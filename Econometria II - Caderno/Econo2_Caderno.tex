\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}

\documentclass[11pt, a4paper]{report}
\usepackage[utf8]{inputenc}
%\usepackage{natbib}
\usepackage{graphicx}
\usepackage[left=2.5cm,top=2.5cm,right=2.5cm,bottom=4cm]{geometry}
%\usepackage[none]{hyphenat}
%\usepackage{showframe}
\usepackage{ragged2e}
\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{sectsty}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{floatrow}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{mathtools}
\usepackage{tikz, tkz-base, tkz-fct}
\usepackage{mathdesign}
\usepackage{cancel}
\usepackage{dirtytalk}
\usepackage{pgfplots}
\usepackage{tikz}

% KnitR

\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
\usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
\usepackage{unicode-math}
\defaultfontfeatures{Scale=MatchLowercase}
\defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
	\usepackage[]{microtype}
	\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
	\IfFileExists{parskip.sty}{%
		\usepackage{parskip}
	}{% else
		\setlength{\parindent}{0pt}
		\setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
	\KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\urlstyle{same} % disable monospaced font for URLs
%\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\},fontsize=\scriptsize}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=0.6\maxwidth,height=0.6\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{h!}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
	\setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\renewcommand{\baselinestretch}{1.0}

\definecolor{coolblack}{rgb}{0.0, 0.18, 0.39}
\hypersetup{colorlinks, breaklinks,
	linkcolor=coolblack,
	filecolor=[rgb]{0.19, 0.55, 0.91},      
	urlcolor=[rgb]{0.19, 0.55, 0.91},
	anchorcolor=[rgb]{0.19, 0.55, 0.91},
	citecolor=black}

\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand{\nindep}{\not\!\perp\!\!\!\perp}

\let\origfigure\figure
\let\endorigfigure\endfigure



\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}

\theoremstyle{plain}
\newtheorem{defn}{Definition}[section]
\newtheorem{conj}{Conjecture}[section]
\newtheorem{exmp}{Example}[section]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}





\title{\Huge \textbf{Econometrics II} \\ \LARGE Notes - Midterm}
\author{\Large William Radaic Peron}
\affil{\Large EESP-FGV}

%\sectionfont{\fontsize{12}{12}\selectfont}
%\chapterfont{\fontsize{16}{12}\selectfont}
%\subsectionfont{\fontsize{11}{11}\selectfont}

\begin{document}
\setlength{\parindent}{0em}
%\setlength{\parskip}{0.6em}
\maketitle

\tableofcontents

\chapter{Introduction}

\section{Motivation}

This course will be dedicated to \textit{time series analysis.} Informally, a \textit{time series} is any type of data collected over time -- or, more formally, it is the realization of a stochastic process indexed in time. We usually denote the time series as follows: 
$$ y_1, ..., y_T; \hspace{1em} \{y_t\}_{i=1}^T; \hspace{1em} \{y_t\}_t $$

Time series analysis is useful for a number of different applications: 
\begin{itemize}
	\item \textbf{Forecasting.} \begin{itemize}
		\item Uni and multivariate models 
		\item \textbf{ARIMA} models: mean and confidence interval forecasting
		\item \textbf{ARCH} models: variance forecasting -- especially useful in finance for volatility and risk
	\end{itemize}
	\item \textbf{Dynamics.} Evaluate the impact of one variable in another over time. \begin{itemize}
		\item Multivariate models including VAR, ECM
		\item Contemporaneous lagged structural relations
	\end{itemize}
\end{itemize}

It is important to address a first and simple question. \textbf{Why time series are different from other data?} The answer is also simple but incredibly relevant: \textit{time series observations are not serially independent!}
$$ Y_t \nindep Y_{t-j} $$
In fact, they don't even have to be identically distributed:
$$ F_{Y_t} \neq F_{Y_{t-j}} $$
This means that the essential \textit{iid} hypothesis for traditional Econometrics \textit{does not hold.} This means that we'll have to make some adjustments to our methods. That is the task of time series analysis.

\section{Statistics with dependence}

Let's begin with a proper definition of a time series. 

\subsection{Definition of a time series}

Suppose that we have a probability space $(\Omega, S, \mathbb{P})$. $\Omega$ is the sample space; $S$ is the set of all events; $\mathbb{P}$ is a measure of probability $\mathbb{P}: S \rightarrow [0,1]$. From this, we define a random variable $Y: \Omega \rightarrow \mathbb{R}$. A realization of this r.v. is denoted by $y = Y(\omega)$ with fixed $\omega$.

From this, we can define multiple random variables in the same sample space, indexed by integers:
$$	Y = \{..., Y_{t-2}, Y_{t-1}, Y_t, ...\} $$
This is equivalent to writing:
$$ Y: \Omega x \mathbb{Z} \rightarrow \mathbb{R}$$

We now arrive at our formal definition of a time series: $\{Y_t, t \in \mathbb{Z} \}$ is a time-indexed stochastic process.
\begin{itemize}
	\item $Y(\cdot, t): \Omega \rightarrow \mathbb{R}$ is a r.v. for fixed $t$.
	\item $Y(\omega, \cdot): \mathbb{Z} \rightarrow \mathbb{R}$ is a \textit{sequence of real numbers} for a fixed $\omega$. In other words, this represents the \textit{observed time series.}
	\item For fixed $t, \omega$, $Y(\omega, t) \in \mathbb{R}$.
\end{itemize}

\subsection{Unconditional expectation}

An important concept to make clear here is \textit{unconditional expectation.} With fixed $t$, 
$$ \mathbb{E}(Y_t) = \int_{-\infty}^{\infty} x f_{Y_t}(x) dx $$
Note the $Y_t$ subscript on the probability density function $f_{Y_t}$. This means that $\mathbb{E}(Y_t)$ is not calculated with the values assumed by $Y_{t-1}, Y_{t+1}$. This raises an important problem: \textit{how would you be able to estimate $\mathbb{E}(Y_t)$?} Note that we only observe $Y_t = y_t$, i.e., one realization of the r.v.


\subsection{Statistical dependence}

For any random variables $X, Y$, we can define multiple measures of dependency:
\begin{itemize}
	\item \textbf{Linear:} $Cov(X,Y) \equiv \mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) $
	\item \textbf{Quadratic:} $Cov(X^2, Y^2)$
	\item \textbf{General:} $Cov(f(X), g(Y))$. This is a measure of covariance between two general functional forms of $X$ and $Y$.
\end{itemize}
With this general definition, we arrive at an equivalent definition for independent random variables:
\begin{itemize}
	\item $F_{X,Y}(x,y) = F_X(x) * F_Y(y),$ i.e., joint pdf is equal to the product of the marginal pdfs.
	\item $Cov(f(X), g(Y)) = 0$ for every pair of bounded functions $f, g$.
\end{itemize}

From this, we now define the \textit{autocovariance and autocorrelation functions.}

\begin{defn}
	$\gamma_{j,t} := Cov(Y_t, Y_{t-j})$ is the \textbf{autocovariance function} for a given time series $\{Y_t, t \in \mathbb{Z} \}.$
\end{defn}

\begin{defn}
	$\rho_{j,t} := \dfrac{\gamma_{j,t}}{\sqrt{\gamma_{0,t}\gamma_{0,t-j}}}$ is the \textbf{autocorrelation function} for a given time series $\{Y_t, t \in \mathbb{Z} \}.$
\end{defn}

Note that, if \textit{iid} holds:
$$ \gamma_{j,t} = \begin{cases}
	0 & j \neq 0, \forall t \\
	Var(Y) & otherwise 
\end{cases} $$

This is an example of an autocorrelation function.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{"acf example"}
	%\caption{}
	\label{fig:acf-example}
\end{figure}

\section{Asymptotic theory with dependence}

Some form of asymptotic theory is needed to enable \textit{any kind of statistical analysis.} Namely, we need to have some form of Law of Large Numbers (LLN) and Central Limit Theorem (CLT) that are analogous to the \textit{iid} environment. This will be achieved in our setting with some conditions called \textit{stationarity} and \textit{ergodicity.}

\subsection{Stationarity}

\begin{defn}
	A process $\{Y_t, t \in \mathbb{Z} \}$ is \textbf{strictly stationary} if, for all finite set of indexes $\{t_1, ..., t_r\}$ and for all $m \in \mathbb{Z}$, $F(y_{t_1},..., y_{t_r}) = F(y_{t_1+m},..., y_{t_r+m})$ holds, where $F(y_{t_1},..., y_{t_r})$ is the joint cdf of $(Y_{t_1},..., Y_{t_r}).$
\end{defn}

More informally, a given process is called \textit{strictly stationary} if its statistical properties depend only on the \textit{relative position} between observations, and not its \textit{absolute position.}

We'll usually adopt a weaker definition of stationarity for our models. Henceforth, we will refer to stationarity in this sense.

\begin{defn}
	A process $\{Y_t, t \in \mathbb{Z} \}$ is \textbf{stationary} (or weakly stationary) if there exists $\mu \in \mathbb{R}$ and $\{\gamma_j\}_{j \in \mathbb{N}}$ such that:
	\begin{itemize}
		\item $\mathbb{E}(Y_t) = \mu, \hspace{1em} \forall t$
		\item $\mathbb{E}[(Y_t - \mu)(Y_{t-j}-\mu)] = \gamma_j, \hspace{1em} \forall (t,j) \in \mathbb{N}^2$
	\end{itemize}
\end{defn}

	Note that, from the second condition in the definition, we have $\mathbb{E}(Y_t - \mu)^2 = \gamma_0 \in \mathbb{R}, \forall t \in \mathbb{N}.$ In other words, \textit{the unconditional variance of the time series is constant.}

\vspace{1em}

Some important remarks on stationarity: 
\begin{itemize}
	\item Stationarity does not imply strict stationarity
	\item Stricy stationarity does not imply stationarity
	\item Every strictly stationary process with finite variance is stationary
	\item Every iid process is strictly stationary
	\item Every strictly stationary process is identically distributed
	\item A stationary process is not necessarily identically distributed
\end{itemize}

\subsection{Ergodicity}

Stationarity is not enough to guarantee that we have even a Law of Large Numbers. To see why that is the case, consider the following example:
$$ Y_t = X + \varepsilon_t, \hspace{1em} \varepsilon \sim \mathcal{N}(0, \sigma^2), \hspace{1em} X \sim \mathcal{N}(0,1), \hspace{1em} X \indep \varepsilon_t $$

Is this process stationary? No, because the sample \textit{time average} $\bar{y} = \frac{1}{T}\sum_{t=1}^T y_t$ does not converge to the population \textit{ensemble average} $\mathbb{E}(Y_t) = \mu$.

We need some condition that guarantees that the dependence structure of the time series decays as the observation get further from each other. That is the intuition behind \textit{ergodicity.}

\begin{defn}
	A strictly stationary process $\{Y_t, t \in \mathbb{Z} \}$ is called \textbf{ergodic} if
	$$ \lim_{J \rightarrow \infty} \dfrac{1}{J} \sum_{j=1}^J Cov[f(X_1), g(X,j)] = 0, $$
	for all pairs of bounded functions $f, g$.
\end{defn}

This is a kind of mean asymptotic independence, in which the asymptotic independence would be defined by $Cov[f(X_1), g(X_J)] \rightarrow 0$ as $J \rightarrow \infty$. 

Now, we can define a Law of Large Numbers -- also called the \textit{Ergodic Theorem}.

\begin{thm}
	Given an ergodic stochastic process $\{Y_t, t \in \mathbb{Z} \}$ such that $\mathbb{E}|Y_1| < \infty$,
	$$\lim_{T \rightarrow \infty} \dfrac{1}{T} \sum_{t=1}^T Y_t = \mathbb{E}(Y_1) \hspace{1em} almost \, sure$$
\end{thm}

This theorem is the generalization of the strong LLN. However, it presupposes \textit{strict stationarity,} which is a very strong assumption most of the time. Fortunately, this theorem gave rise to other definitions that arrive at our objective, namely, a LLN for the first two moments.

\begin{defn}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is said to be \textbf{ergodic for the mean} if
	$$ \dfrac{1}{T} \sum_{t=1}^T Y_t \to_p \mathbb{E}(Y_t), \hspace{1em} T \to \infty $$
\end{defn}

\begin{defn}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is said to be \textbf{ergodic for the second moment} if, for every $j,$
	$$ \dfrac{1}{T-j} \sum_{t=j+1}^T Y_t Y_{t-j} \to_p \mathbb{E}(Y_t), \hspace{1em} T \to \infty $$
\end{defn}

\begin{prop} \label{sum-ergodicity}
	$\sum_{j=0}^{\infty} |\gamma_j| < \infty $ is a suficient condition for ergodicity for the mean.
\end{prop}

\begin{proof}
	Let $Z_t := Y_t - \mu$ and $\bar{Z_t} := \frac{1}{T} \sum_{t=1}^T Z_t$, where $\{Y_t, t \in \mathbb{Z} \}$ is a stationary process. We will show that $\bar{Z_t}$ converges to 0 in mean square.
	
	\begin{math}
		\begin{aligned}
			\mathbb{E}\left(\bar{Z}_{T}^{2}\right) &=\mathbb{E}\left[\left(\frac{1}{T} \sum_{t=1}^{T} Z_{t}\right)\left(\frac{1}{T} \sum_{t=1}^{T} Z_{t}\right)\right]=\frac{1}{T^{2}} \mathbb{E}\left(\sum_{s=1}^{T} \sum_{t=1}^{T} Z_{s} Z_{t}\right) \\
			&=\frac{1}{T^{2}} \sum_{s=1}^{T} \sum_{t=1}^{T} \mathbb{E}\left(Z_{s} Z_{t}\right)=\frac{1}{T^{2}} \sum_{s=1}^{T} \sum_{t=1}^{T} \gamma_{s-t}=\frac{1}{T} \sum_{j=-T+1}^{T-1} \frac{T-|j|}{T} \gamma_{j} \\
			& \leq \frac{1}{T} \sum_{j=-T+1}^{T-1} \frac{T-|j|}{T}\left|\gamma_{j}\right| \leq \frac{1}{T} \sum_{j=-T+1}^{T-1}\left|\gamma_{j}\right| \rightarrow 0
		\end{aligned}
	\end{math} 

\end{proof}

\subsection{A Central Limit Theorem for time series}

The conditions that guarantee the existence of a CLT for stationary and ergodic processes are much more envolving than in the \textit{iid} environment. However, we have a relatively simple result that will be useful to us in time series analysis. It will now be presented without proof.

\begin{thm}
	Let $\{Y_t, t \in \mathbb{Z} \}$ be a \textbf{linear} stationary process, i.e., that can be written in the form $Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \epsilon_{t-j}$, where $\varepsilon \sim_{iid} (0, \sigma^2)$ and $\sum_{j = -\infty}^{\infty} | \psi_j | < \infty$. Then,
	$$ \sqrt{T}(\bar{Y_t}-\mu) \to_d \mathcal{N}(0, \omega^2), $$ 
	where $\omega^2 := \sum_{j = -\infty}^{\infty} \gamma_{j} < \infty $	
\end{thm}

\chapter{ARMA Models}

ARMA is a class of models that we'll employ frequently in time series analysis. Let's begin with some definitions.

\section{White noise}

We call \textit{white noise} stationary time series with mean zero that do not have serial correlation. 

\begin{defn}
	$\{Y_t, t \in \mathbb{Z} \}$ is \textbf{white noise}, denoted by $Y_t \sim wn(0,\sigma^2)$, if
	$$\mathbb{E}\left(Y_{t}\right)=0 ; \quad \mathbb{E}\left(Y_{t}, Y_{t-j}\right)=\left\{\begin{array}{ll}
		\sigma^{2} & j=0 \\
		0 & j \neq 0
	\end{array}\right.$$
\end{defn}

This is the most simple time series -- except for the \textit{iid} case, where independence also holds. It will be the building block for a number of processes that we will study.

\section{Moving Average processes}

Let's begin with the simplest form of MA processes: MA(1). 
\begin{defn} \label{ma1-def}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is called \textbf{MA(1)}, or a \textbf{moving average of order 1}, if it follows the following form:
	$$ Y_t = c + \varepsilon_t + \theta \varepsilon_{t-1}, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2), $$
	where $c, \theta$ are constant.
\end{defn}

\subsection{Moments of an MA(1) model}

The expected value of an MA(1) is:
$$
\mu \equiv \mathbb{E}\left(Y_{t}\right)=\mathbb{E}\left(c+\varepsilon_{t}+\theta \varepsilon_{t-1}\right)=c
$$

With this result, we can rewrite the model as: 
$$
\left(Y_{t}-\mu\right)=\varepsilon_{t}+\theta \varepsilon_{t-1}
$$

Multiplying both sides by $\left(Y_{t-j}-\mu\right)$ yields:
$$
\begin{aligned}
	\left(Y_{t}-\mu\right)\left(Y_{t-j}-\mu\right) &=\left(\epsilon_{t}+\theta \epsilon_{t-1}\right)\left(\epsilon_{t-j}+\theta \epsilon_{t-j-1}\right) \\
	&=\epsilon_{t} \epsilon_{t-j}+\theta \epsilon_{t} \epsilon_{t-j-1}+\theta \epsilon_{t-1} \epsilon_{t-j}+\theta^{2} \epsilon_{t-1} \epsilon_{t-j-1}
\end{aligned}
$$

Applying the expected value operator to both sides, we have the autocovariances of the model.
$$
\gamma_{j} \equiv \mathbb{E}\left[\left(Y_{t}-\mu\right)\left(Y_{t-j}-\mu\right)\right]=\left\{\begin{array}{ll}
	\left(1+\theta^{2}\right) \sigma^{2} & j=0 \\
	\theta \sigma^{2} & j=\pm 1 \\
	0 & |j|>1
\end{array}\right.
$$

\subsection{Some examples of MA(1) processes}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{ma1}
	\label{fig:ma1}
\end{figure}

\subsection{Deriving the Autocorrelation function of the MA(1) process}

While deriving the moments of the MA(1), it became clear that the process is stationary and ergodic to the mean. Note that the time average $\bar{y_t}$ converges to $\mathbb{E}(Y_t)$, the \textit{ensemble average}, the absolute sum of all covariances is clearly finite ($\gamma_j = 0, \forall j > 1$) and the dependence structure depends only on the relative positions of the observations.

Let's use the results of the autocovariances to construct the ACF:

$$\rho_{j} \equiv \frac{\gamma_{j}}{\gamma_{0}}=\left\{\begin{array}{ll}
	1 & j=0 \\
	\frac{\theta}{1+\theta^{2}} & j=\pm 1 \\
	0 & |j|>1
\end{array}\right.$$

Note that the ACF of an MA(1) process is \textit{truncated} in zero for lags greater than 1.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{"fac facp"}
	\label{fig:fac-facp}
\end{figure}

\section{Generalizing the MA model}

We can now generalize the MA(1) model for a moving average of order $q$.

\begin{defn}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is called \textbf{MA(q)}, or a \textbf{moving average of order q}, if it follows the following form:
	$$ Y_t = c + \varepsilon_t + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q}, \hspace{1em} \varepsilon \sim wn(0, \sigma^2), $$
	where $c, \theta_1, ..., \theta_q \in \mathbb{R}, q \in \mathbb{Z}^+$.
\end{defn}

\subsection{Moments of an MA(q) process}

The expected value of an MA(q) is:
$$
\mu \equiv \mathbb{E}\left(Y_{t}\right)=\mathbb{E}\left(c+\varepsilon_{t}+\theta_{1} \varepsilon_{t-1}+\cdots+\theta_{q} \varepsilon_{t-q}\right)=c
$$

Again, using the first result, we can rewrite the model as:
$$
\left(Y_{t}-\mu\right)=\varepsilon_{t}+\theta_{1} \varepsilon_{t-1}+\cdots+\theta_{q} \varepsilon_{t-q}
$$

Multiplicando ambos os lados por $\left(Y_{t-j}-\mu\right),$ temos
$$
\begin{aligned}
	\left(Y_{t}-\mu\right)\left(Y_{t-j}-\mu\right) &=\left(\sum_{k=0}^{q} \theta_{k} \varepsilon_{t-k}\right)\left(\sum_{k=0}^{q} \theta_{k} \varepsilon_{t-j-k}\right) \\
	&=\sum_{k=0}^{q} \sum_{\ell=0}^{q} \theta_{k} \theta_{\ell} \varepsilon_{t-k} \varepsilon_{t-j-\ell},
\end{aligned}
$$

where $\theta_{0}=1 .$ Applying the expectation operator, we have:
$$
\gamma_{j}=\left\{\begin{array}{ll}
	\left(\theta_{j}+\theta_{j+1} \theta_{1}+\theta_{j+2} \theta_{2}+\cdots+\theta_{q} \theta_{q-j}\right) \sigma^{2} & |j|=0,1, \ldots, q \\
	0 & |j|>q
\end{array}\right.
$$

\subsection{Deriving the Autocorrelation function of the MA(q) process}

Again, we can clearly see that the MA(q) model is \textit{stationary} and \textit{ergodic.} Note that the time average $\bar{y_t}$ converges to $\mathbb{E}(Y_t)$, the \textit{ensemble average}, the absolute sum of all covariances is clearly finite ($\gamma_j = 0, \forall j > q$) and the dependence structure depends only on the relative positions of the observations.

The autocorrelation function is given by:
$$\rho_{j} \equiv \frac{\gamma_{j}}{\gamma_{0}}=\left\{\begin{array}{ll}
	1 & j=0 \\
	\frac{\theta_{j}+\theta_{j+1} \theta_{1}+\theta_{j+2} \theta_{2}+\cdots+\theta_{q} \theta_{q-j}}{1+\theta_{1}^{2}+\cdots+\theta_{q}^{2}} & |j|=1,2, \ldots, q \\
	0 & |j|>q
\end{array}\right.$$

Now, the ACF is truncated in zero for lags \textit{greater than $q$.}


\section{The MA($\infty$) model}

Consider a special case of a MA(q) model where $q \to \infty$. This yields a moving average of infinite order, MA($\infty$).

\begin{defn}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is called \textbf{MA($\infty$)}, or a \textbf{moving average of infinite order}, if it follows the following form:
	$$ Y_t = c + \sum_{i=0}^{\infty} \theta_i \varepsilon_{t-i}, \hspace{1em} \sim wn(0, \sigma^2), $$
	where $c, \theta_1, ..., \theta_q \in \mathbb{R}, \theta_0 = 1$.
\end{defn}

We also assume that $\sum_{i=0}^{\infty} | \theta_i | < \infty $. This guarantees that the process is \textit{ergodic.}\footnote{Details in Hamilton (1994), Appendix, 3.A.} With this assumption, we can obtain the moments of the MA($\infty$) simply by taking the limit of the finite case MA(q) -- because it enables us to exchange the order between the sum and the expectation operator.

This means that $\mu = c$, as in the previous cases, and:
$$\gamma_{j}=\left(\sum_{i=0}^{\infty} \theta_{j+i} \theta_{i}\right) \sigma^{2}$$

\section{The Wold Decomposition}

This result motivates all ARMA models. It can be defined informally as ``any stationary process has a MA($\infty$) representation''.

\begin{thm}{\textbf{Wold Representation Theorem.}}
	Any process $\{Y_t, t \in \mathbb{Z} \}$ purely nondeterministic can be written as
	$$ Y_t = \mu + \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j}, $$
	where $\varepsilon_{t} = Y_t - \pi(Y_t | 1, Y_{t-1}, Y_{t-2}, ...)$, i.e., $\varepsilon_{t}$ is the \textit{error of the linear projection of $Y_t$ in $(1, Y_{t-1}, Y_{t-2}, ...)$}.
\end{thm}

\section{Autoregressive models}

Again, we'll begin with its simplest form, AR(1).

\begin{defn} \label{ar1-def}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is called \textbf{AR(1)}, or an \textbf{autoregressive process of order 1}, if it follows the following form:
	$$ Y_t = c + \phi Y_{t-1} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2), $$
	where $c, \theta$ are constant.
\end{defn}

\subsection{Moments of an AR(1) process}

With AR($\cdot$) models, we will work in the opposite direction when it comes to stationarity. We'll first \textit{assume} that is holds, and then provide reasoning for why the assumption is valid.

With the assumption of stationarity, we can take expectations and variances on both sides:
$$ \mu = c + \phi \mu \iff \mu = \mathbb{E}(Y_t) = \dfrac{c}{1 - \phi} $$
$$ \gamma_{0} = \phi^2 \gamma_{0} + \sigma^2 \iff \gamma_{0} = Var(Y_t) = \dfrac{\sigma^2}{1 - \phi^2} $$

Using the first result, we can rewrite the model as:
$$ (Y_t - \mu) = \phi (Y_{t-1} - \mu) + \varepsilon_{t} $$

Multiplying both sides by $(Y_{t-j} - \mu)$ and taking expectations yields:
$$ \gamma_{j} = \phi \gamma_{t-j}, \hspace{1em} = 1,2,... $$

\subsection{Some examples of AR(1) series}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{"ar1"}
	\label{fig:fac-facp}
\end{figure}

\subsection{Autocorrelation function of an AR(1) process}

Given that $\gamma_{j} = \phi \gamma_{t-j}$, it is easy to see that the autocovariance is given by:
$$ \gamma_j = \phi^{|j|} \gamma_0, \hspace{1em} j \in \mathbb{Z} $$

Therefore, $\rho_{j} = \dfrac{\gamma_{j}}{\gamma_{0}} = \phi^{|j|}$.

\subsection{Partial Autocorrelation Function}

Note that $Y_t, Y_{t-2}$ \textit{are correlated.} Can we isolate the correlation between $Y_t, Y_{t-2}$ from the effects of $Y_{t-1}$? 
$$ Cor(Y_t, Y_{t-2} | Y_{t-1}) = Cor(c + \phi Y_{t-1} + \varepsilon_{t}, Y_{t-2} | Y_{t-1}) = 0 $$

This is the intuition behind the \textit{partial autocorrelation function} (PACF). 

\begin{defn}
	The \textbf{partial autocorrelation function} of a stationary process $\{Y_t, t \in \mathbb{Z} \}$ is given by:
$$\alpha_{j}=\left\{\begin{array}{ll}
	\operatorname{Cor}\left(Y_{t}, Y_{t-1}\right)=: \rho_{1} & j=1 \\
	\operatorname{Cor}\left(Y_{t}, Y_{t-j} \mid Y_{t-1}, \ldots, Y_{t-j+1}\right) & j \geq 2
\end{array}\right.$$
\end{defn}

To estimate the ACF of a given time series, we need to use its sample equivalent and a version of the Law of Large Numbers, presented in the previous section, because we're only looking for correlations -- i.e., population moments. To estimate the PACF, that is not enough. We're now looking for \textit{partial correlation.} 

It so happens that OLS gives us the \textit{ceteris paribus} effects. Note that a general form for $\beta$ is given by: $ \beta = \frac{Cov(X,Y)}{Var(X)}$. Therefore, we can estimate using OLS the following models for every $j$: 
$$ Y_t = \beta_0 + \beta_1 Y_{t-1} + ... + \alpha_j Y_{t-j} + u_t $$
The last coefficient of \textit{each regression}, $\hat{\alpha}_j$, is a consistent estimator for $\alpha_{j}$. It is important to highlight, here, that a new model shall be estimated \textit{for each $j$}, as it guarantees that the coefficient $\alpha_{j}$ will be conditional on all $t$ \textit{prior} to $j$.

The following plots showcase ACFs and PACFs for AR(1) processes.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{"fac facp ar1"}
	\label{fig:fac-facp-ar1}
\end{figure}

\subsection{Conditions for stationarity}

When is an AR(1) process stationary? Note that:

$$
\begin{aligned}
	Y_{t} &=c+\phi Y_{t-1}+\varepsilon_{t} \\
	&=c+\phi\left(c+\phi Y_{t-2}+\varepsilon_{t-1}\right)+\varepsilon \\
	&=c+\phi\left(c+\phi\left(c+\phi Y_{t-3}+\varepsilon_{t-2}\right)+\varepsilon_{t-1}\right)+\varepsilon_{t} \\
	& \cdots \\
	&=c \sum_{j=0}^{k-1} \phi^{j}+\phi^{k} Y_{t-k}+\sum_{j=0}^{k-1} \phi^{j} \varepsilon_{t-j}
\end{aligned}
$$

Assuming that $|\phi|<1$ and taking the limit $k \rightarrow \infty,$ we have:

$$
Y_{t}=\frac{c}{1-\phi}+\sum_{j=0}^{\infty} \phi^{j} \varepsilon_{t-j}
$$

The first term follows from the sum of an infinite geometric sequence. This means that \textit{an AR(1) process can be written as a MA($\infty$)} with $\sum_{j = 0}^\infty | \theta_j | < \infty$. Note that this is equivalent to saying that the Wold Representation Theorem holds, with $\mu = \frac{c}{1 - \phi}$, $\psi_j = \phi^j$. This guarantees that the AR(1) process is \textit{stationary} and \textit{ergodic.}

\section{Generalizing the AR model}

\begin{defn}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is called \textbf{AR(p)}, or an \textbf{autoregressive process of order p}, if it follows the following form:
	$$ Y_t = c + \phi Y_{t-1} + ... + \phi_p Y_{t-p} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2), $$
	where $c, \theta_1, ..., \theta_p$ are constant.
\end{defn}

\subsection{Moments of an AR(p) process}

Assuming stationarity, we can apply again expectations on both sides:
$$
\mu=c+\phi_{1} \mu+\cdots+\phi_{p} \mu \Longleftrightarrow \mu=\frac{c}{1-\phi_{1}-\cdots-\phi_{p}}
$$

Using this result, we can rewrite the model as:
$$
\left(Y_{t}-\mu\right)=\phi_{1}\left(Y_{t-1}-\mu\right)+\cdots+\phi_{p}\left(Y_{t-p}-\mu\right)+\epsilon_{t}
$$

Multiplying both sides by $\left(Y_{t-j}-\mu\right)$ and taking expectations, we have:
$$
\gamma_{j}=\left\{\begin{array}{ll}
	\phi_{1} \gamma_{j-1}+\cdots+\phi_{p} \gamma_{j-p} & j=1,2, \ldots \\
	\phi_{1} \gamma_{1}+\cdots+\phi_{p} \gamma_{p}+\sigma^{2} & j=0
\end{array}\right.
$$
Note that the last term in $\gamma_{0}$ is implied by $\mathbb{E}\left(\epsilon_{t}\right)\left(Y_{t}-\mu\right)=\sigma^{2}$.

\subsection{ACF of an AR(p) process}

Dividing the previous result by $\gamma_{0}$ yields:
$$ \rho_j = \phi_1 \rho_{j-1} + ... + \phi_p \rho_{j-p} $$

Evaluating at $j = 1,2,..., p - 1$ and using $p_i = p_{-i}$, we have the following system of difference equations (aka. Yule-Walker Equations):
$$\left\{\begin{array}{l}
	\rho_{1}=\phi_{1}+\phi_{2} \rho_{1}+\cdots+\phi_{p} \rho_{p-1} \\
	\rho_{2}=\phi_{1} \rho_{1}+\phi_{2}+\cdots+\phi_{p} \rho_{p-2} \\
	\vdots \\
	\rho_{p}=\phi_{1} \rho_{p-1}+\phi_{2} \rho_{p-2}+\cdots+\phi_{p}
\end{array}\right.$$

To solve this, we need to find $\rho_1, \rho_2, ..., \rho_j$ as functions of $\phi_1, \phi_{2}, ..., \phi_j$. The first equation above implies that further correlations from lag $j$ will decay exponentially\footnote{Review this.}. This means that the ACF pattern of an AR(p) looks like the one from the simple AR(1) model.

\section{The Lag Operator}

\begin{defn}
	Given a process $\{Y_t, t \in \mathbb{Z} \},$ the \textbf{lag operator} is defined by:
$$\begin{array}{l}
	L Y_{t}:=Y_{t-1} \\
	L^{2} Y(t):=L\left(L Y_{t}\right)=L\left(Y_{t-1}\right)=Y_{t-2} \\
	\vdots \\
	L^{j} Y(t):=L\left(L\left(L \ldots L Y_{t}\right)=Y_{t-j}\right.
\end{array}$$
\end{defn}

The lag operator is also commutative with multiplication and distributive with regards to addition:
$$ L(cY_t) = c(LY_t) $$
$$ L(Y_t + X_t) = LY_t + LX_t $$

\subsection{The lag operator as a polynomial}

Note that we can use the lag operator as a \textit{polynomial.} We can now rewrite an AR(p) with zero mean as:
$$ (1 - \phi_1 L - \phi_2 L^2 - ... - \phi_p L^p)Y_t = \varepsilon_{t} $$

Note that the term multiplying $Y_t$ \textit{is a polynomial in L.} We denote this by:
$$ (L) Y_t = \varepsilon_{t} $$

Analogously, we can rewrite an MA(q) process as:
$$\begin{aligned}
	Y_{t} &=\epsilon_{t}+\theta_{1} \epsilon_{t-1}+\theta_{2} \epsilon_{t-2}+\cdots+\theta_{q} \epsilon_{t-q} \\
	&=\left(1+\theta_{1} L+\theta_{2} L^{2}+\cdots+\theta_{q} L^{q}\right) \epsilon_{t} \\
	& \equiv \Theta_{q}(L) \epsilon_{t}
\end{aligned}$$

We would also like to define an operator $(1 - \phi L)^{-1}$ such that:
$$ (1 - \phi L)^{-1} (1 - \phi L) = 1 $$

$(1 - \phi L)^{-1}$ is well defined when $|\phi| < 1$ and the following condition holds\footnote{Hamilton (1994), p. 27-29.}:
$$ (1 - \phi L)^{-1} := 1 + \phi L + \phi^2 L^2 + \phi^3 L^3 + ... $$

From this, we can rewrite the AR(1) as a MA($\infty$) by multiplying the AR by $(1 - \phi L)^{-1}$ on both sides:
$$ Y_t = (1 - \phi L)^{-1} \varepsilon_{t}$$

The $(1 - \phi L)^{-1}$ operator will be very useful to translate models between AR and MA representations, aside from highlighting the conditions of stationarity for the process.


\subsection{Stationarity and the lag operator}

We can factor out the polynomial of an AR(p) process as:
$$ 1 - \phi_1 L - ... - \phi_{p} L^p = (1 - \lambda_1 L)...(1 - \lambda_p L), $$
where $\lambda_j = \frac{1}{a_j} \forall j = 1,..., p$ and $a_1, ..., a_p$ are the $p$ roots of a polynomial of $p$-th degree. This means that we can rewrite the AR(p) process as:
$$ (1 - \lambda_1 L)...(1 - \lambda_p L) = \varepsilon_{t} $$

If $|\lambda_p | < 1$ (or, equivalently, $|a_j > 1$) $\forall j = 1, ..., p$, then \textit{the inverse polynomial exists} and we can write the AR(p) process as a MA($\infty$) -- which \textit{we know to be stationary}:
$$\begin{aligned}
	Y_{t} &=\left(1-\lambda_{1} L\right)^{-1} \ldots\left(1-\lambda_{p} L\right)^{-1} \epsilon_{t} \\
	&=:\left(1+\psi_{1} L+\psi_{2} L^{2}+\ldots\right) \epsilon_{t} \\
	&=: \Psi_{\infty}(L) \epsilon_{t}
\end{aligned}$$

\section{Finally, the ARMA(p,q) process}

An ARMA(p,q) model is created by combining an AR(p) with a MA(q).

\begin{defn}
	A stationary process $\{Y_t, t \in \mathbb{Z} \}$ is called \textbf{ARMA(p,q)}, or an \textbf{autoregressive-moving average process of order (p,q)}, if it follows the following form:
	$$ Y_t = c + \phi Y_{t-1} + ... + \phi_p Y_{t-p} + \theta_1 \varepsilon_{t-1} + ... + \theta_q \varepsilon_{t-q} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2), $$
	where $c, \theta_1, ..., \theta_p, \phi_1, ..., \phi_q$ are constant, $p, q \in \mathbb{Z}^+$.
\end{defn}

Using the lag operator yields an alternate form for the ARMA(p,q) process:
$$\begin{aligned}
	\left(1-\phi_{1} L-\cdots-\phi_{p} L^{p}\right) Y_{t} &=c+\left(1+\theta_{1} L+\cdots+\theta_{q} L^{q}\right) \epsilon_{t} \\
	\Phi_{p}(L) Y_{t} &=c+\Theta_{q}(L) \epsilon_{t}
\end{aligned}$$

\subsection{Stationarity and invertibility of an ARMA(p,q) process}

\textbf{Stationarity} depends only on the AR part of the process, because all MA($\cdot$) are stationary. It is sufficient to verifiy that \textit{the roots of the polynomial $\Phi_p(L)$ are out of the unit circle:}
$$\Phi_p(L) = 1 - \phi_1 L - \phi_2 L^2 - ... - \phi_{p} L^p $$

\textbf{Invertibility} depends only on the MA part of the process, because it needs to be able to be rewritten as a linear combination of its past values plus the contemporaneous error term $\varepsilon_{t}$:
$$ Y_t = \alpha + \sum_{s=1}^{\infty} \pi Y_{t-s} + \varepsilon_{t} $$
for some $\alpha$ and \{$\pi_j$\}. 

\begin{quote}
Consider, for example, the case of $M A(1)$ with $\mu=0$
$$
y_{t}=\varepsilon_{t}+\theta \varepsilon_{t-1}
$$
which can be rewritten as
$$
\varepsilon_{t}=y_{t}-\theta \varepsilon_{t-1}
$$
Repeated substitution of this relation for the lagged $\varepsilon_{t-s}$ terms yields
$$
\begin{aligned}
	\varepsilon_{t} &=y_{t}-\theta\left(y_{t-1}-\theta \varepsilon_{t-2}\right) \\
	&=y_{t}-\theta y_{t-1}+\theta^{2} \varepsilon_{t-2} \\
	& \cdots \\
	&=y_{t}-\theta y_{t-1}+\ldots+(-\theta)^{p} y_{t-p}+(-\theta)^{p+1} \varepsilon_{t-p+1}
\end{aligned}
$$
If $|\theta|<1,$ then the last term in this expression tends to zero in mean-square as $p \rightarrow \infty,$ so that it make
sense to write
$$
\varepsilon_{t}=y_{t}+\sum_{s=1}^{\infty}(-\theta)^{s} y_{t-s}
$$
Or
$$
y_{t}=\varepsilon_{t}+\sum_{s=1}^{\infty}(-\theta)^{s} y_{t-s}
$$
so $|\theta|<1$ is the sufficient condition for a $M A(1)$ process to be invertible. (Powell, Conditions for Stationarity and Invertibility, UC Berkeley.)
\end{quote}

In other words, because AR($\cdot$) models \textit{with roots of the polynomial outside of the unit circle} are invertible, being able to write the MA(q) part of the process as an AR($\infty$) with the root condition is sufficient to guarantee invertibility.

\subsection{Moments of an ARMA(p,q) process}

If the process is stationary, $\Phi_{p}^{-1}(L)$ exists and we can rewrite ARMA $(\mathrm{p}, \mathrm{q})$ as $\mathrm{MA}(\infty)$
$$
Y_{t}=\mu+\Psi_{\infty}(L) \epsilon_{t}
$$
onde
$$
\mu \equiv \frac{c}{\Phi(1)} ; \quad \Phi(1)=1-\sum_{j=1}^{p} \phi_{j} ; \quad \Psi_{\infty}(L) \equiv \Phi_{p}(L)^{-1} \Theta_{q}(L)
$$
From the results derived for MA(q) we have for $q=\infty$
$$
\begin{aligned}
	\mathbb{E}\left(Y_{t}\right) &=\mu \\
	\gamma_{j} &=\left(\sum_{i=0}^{\infty} \psi_{j+i} \psi_{j}\right) \sigma^{2}
\end{aligned}
$$
where $\psi_{0}=1$

\subsection{ACF of an ARMA(p,q) process}

It is usually easy to identify an AR(p) or MA(q) visually by inspecting its ACF and PACF, because AR's PACF is truncated on $p$, MA's ACF is truncated on $q$. For ARMA(p,q) models it is more complicated: both functions are not truncated! Note, however, that in that case, the ACF decays geometrically after lag $q$ and the PACF decays geometrically after lag $p$.

\vspace{1em}

\begin{center}


\begin{tabular}{|c|c|c|}
	\hline
	\textbf{Model}  &\textbf{ ACF }& \textbf{PACF} \\
	\hline
	\textbf{AR($p$)} & Decays & Truncated after lag $p$ \\
	\hline
	\textbf{MA($q$)} & Truncated after lag $q$ & Decays \\
	\hline
	\textbf{ARMA($p,q$)} & Decays after lag $q$ & Decays after lag $p$ \\
	\hline
\end{tabular}


\end{center}

\section{Testing for time dependence} %% Material complementar


We've seen that a sufficient condition for ergodicity is convergence of the absolute sum of all covariances. This presents a problem: how can we \textit{estimate} these covariances?

Let $Z_t$ be our series to be tested. Denote the autocovariance of order $j$ as $\gamma_{j} := Cov(Z_t, Z_{t-j})$. We can try to estimate these parameters with its sample equivalents: 
$$ \bar{z_t} := \dfrac{1}{T} \sum_{t=1}^{T} z_t $$
$$ \hat{\gamma}_{j} := \dfrac{1}{T - j - 1} \sum_{t = j+1}^{T} (z_t - \bar{z_t})(z_{t-j} - \bar{z_t}) $$

But this is not as simple as it seems. We know that $\hat{\gamma}_{j}$ converges almost sure to $\gamma_{j}$ \textit{if the process is ergodic.} If it \textit{isn't,} the information from $\hat{\gamma}_{j}$ may not be reliable -- after all, we won't have a Law of Large Numbers! 

Our solution to this problem won't be very rigorous here. We'll plot $\{\hat{\gamma}_{j}, j \in \mathbb{N}\}$ and check if it looks stationary. If the series passes this intuitive test, we can assume that $\{\hat{\gamma}_{j}, j \in \mathbb{N}\}$ will be informative about $\{{\gamma}_{j}, j \in \mathbb{N}\}$. 

The visual inspection should focus on two main factors: (i) constant mean over time; (ii) constant variance over time. Here are some examples of series to be inspected:

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.6\linewidth]{"exemplos ts"}
	\label{fig:exemplos-ts}
\end{figure}

After plotting the time series and assuring that it is well behaved, we can plot its \textit{correlogram:} $\{ \hat{\rho}_{j} := \frac{\hat{\gamma}_{j}}{\hat{\gamma}_{0}}, j \in \mathbb{N}\}$. If its sum looks convergent, we will assume that the process is \textit{stationary} and \textit{ergodic} -- which will enable us to use sample equivalents as representations of population parameters.

\subsection{Hypothesis testing}

Consider this correlogram:

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{"correlograma rx"}
	\label{fig:correlograma-rx}
\end{figure}

This series appears to not be correlated with its past. How can we test this? 
$$ H_0 = \rho_j = 0, \forall j \neq 0 $$
This implies, in theory, that we would need to test infinite correlations. In practice, we limit the range to an arbitrary $J$. Let $\hat{\rho} := (\hat{\rho}_1, \hat{\rho}_2, ..., \hat{\rho}_J)^T, \rho = (\rho_1, \rho_2, ..., \rho_J)^T.$ Under the null, $\rho = 0$, and as $T \to \infty$:
$$ \sqrt{T}\hat{\rho} \to \mathcal{N}(0, I_J) $$
The intuition here is that, under $H_0$, $\hat{\rho}$ is a sequence of \textit{iid} variables with mean zero and variance-covariance matrix $I_J$ -- which makes the CLT valid. 

Given this result, we can now create a statistic that does not depend on the multivariate normal distribution. We will square and sum the expression to arrive at a Chi-squared distribution. This enables us to test the hypothesis with a Wald statistic. Under the null:

$$
W_{T}=T \hat{\rho}^{T} \hat{\rho}=T \sum_{j=1}^{J} \hat{\rho_{j}}^{2} \rightarrow \chi_{J}^{2}
$$

\subsection{Testing autocorrelations or regressions?}

Note that inferring about the autocorrelations is intimately related to inferring in a regression of a time series on its past values. This can be understood by remembering the linear projection interpretation of OLS. Ordinary Least Squares estimation \textit{always} reports the parameters of the linear projection of $Y$ in $X$, no matter how the model is specified!

Consider the following model:
$$
\begin{aligned}
	Y_{t} &=\alpha+\beta_{1} Y_{t-1}+\beta_{2} Y_{t-2}+\cdots+\beta_{J} Y_{t_{J}}+\varepsilon_{t} \\
	&=\alpha+X_{t} \beta+\varepsilon_{t}
\end{aligned}
$$
where $\beta:=\left(\beta_{1}, \ldots, \beta_{J}\right)^{T}$ and $X_{t}=\left(Y_{t-1}, \ldots, Y_{t-j}\right) .$ If we define the coefficients of the model above as the parameters of the linear projection of $Y_t$ on the unit vector and $X_t$, $\alpha=\mu_{Y}-\mu_{X} \beta$ where $\mu_{Y}=\mathrm{E}\left(Y_{t}\right)$ and $\mu_{X}=\mathrm{E}\left(X_{t}\right)$

Using this result, we have:
$$
Y_{t}-\mu_{Y}=\left(X_{t}-\mu_{X}\right) \beta+\varepsilon_{t}
$$
This means that $\beta$ can be written as:
$$
\beta=\mathbb{E}\left[\left(X_{t}-\mu_{X}\right)^{T}\left(X_{t}-\mu_{X}\right)\right]^{-1} \mathbb{E}\left[\left(X_{t}-\mu_{X}\right)^{T}\left(Y_{t}-\mu_{Y}\right)\right]=\Gamma^{-1} \gamma
$$
where the matrix $\Gamma^{-1}$ is symmetric with diagonal elements all equal to $\gamma_{1}, \gamma_{2}, \ldots, \gamma_{J-1}$, due to the assumed stationarity. Note that $\mathbb{E}(Y_t - \mu_Y)^2 = \mathbb{E}(Y_{t-j} - \mu_{Y-j})^2 = \gamma_0 \forall j$.

Thus, $\beta=\overrightarrow{0} \Longleftrightarrow \gamma=\overrightarrow{0}$, because $\Gamma$ is a positive definite matrix. This means that \textit{testing $\beta = 0$ is equivalent to testing $\gamma = 0$.}

It is important to highlight that this analysis is based upon the inference of $\gamma_{j}=\mathbb{E}\left(z_{t}-\bar{z}_{T}\right)\left(z_{t-j}-\bar{z}_{T}\right).$ If we were interested in \textit{other types of relations between $Z_t$ and its past, the analysis would have to be adapted} -- for example, $Z_t^2$. It would be necessary to check again for stationarity and ergodicity.


\chapter{Problem 1: Modelling exchange rates}

Loading the database and creating dummy variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df <-}\StringTok{ }\KeywordTok{read_excel}\NormalTok{(}\StringTok{"RS_USD.xlsx"}\NormalTok{)}


\KeywordTok{names}\NormalTok{(df)[}\KeywordTok{names}\NormalTok{(df) }\OperatorTok{==}\StringTok{ "R$/US$"}\NormalTok{] <-}\StringTok{ "p"}

\KeywordTok{names}\NormalTok{(df)[}\KeywordTok{names}\NormalTok{(df) }\OperatorTok{==}\StringTok{ "Variação (em %)"}\NormalTok{] <-}\StringTok{ "delta"}

\KeywordTok{names}\NormalTok{(df)[}\KeywordTok{names}\NormalTok{(df) }\OperatorTok{==}\StringTok{ "Data"}\NormalTok{] <-}\StringTok{ "date"}

\NormalTok{sign <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}

\NormalTok{count <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{2153}\NormalTok{)}

\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(count, df, sign)}
\end{Highlighting}
\end{Shaded}

Before constructing our models, we need to check (intuitively) if the
series at hand is \emph{stationary} and \emph{ergodic}. For this, we're
going to plot the time series, its autocorrelations and partial
autocorrelations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pplot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ p)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"USD/BRL, Price"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
\NormalTok{pplot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{deltaplot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ date, }\DataTypeTok{y =}\NormalTok{ delta)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"USD/BRL, %"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
\NormalTok{deltaplot}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dummyplot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ count, }\DataTypeTok{y =}\NormalTok{ sign)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"USD/BRL, +/-"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{xlim}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{200}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
\NormalTok{dummyplot}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Removed 1953 row(s) containing missing values (geom_path).
\end{verbatim}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# For delta}

\NormalTok{acf_delta <-}\StringTok{ }\KeywordTok{Acf}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acf_test_values <-}\StringTok{ }\NormalTok{acf_delta}\OperatorTok{$}\NormalTok{acf}\OperatorTok{/}\KeywordTok{sd}\NormalTok{(acf_delta}\OperatorTok{$}\NormalTok{acf)}

\KeywordTok{head}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(acf_test_values))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   acf_test_values
## 1      37.9547672
## 2       1.4506537
## 3      -0.4173129
## 4       0.2125873
## 5      -1.7053782
## 6       0.5358210
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{facst <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
    \DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
\NormalTok{faclt <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{, }
    \DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}

\NormalTok{facpst <-}\StringTok{ }\KeywordTok{ggPacf}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{100}\NormalTok{, }
    \DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{facplt <-}\StringTok{ }\KeywordTok{ggPacf}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{, }
    \DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{facst}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-5} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faclt}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-6} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{facpst}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-7} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{facplt}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-8} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{facst2 <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{((df}\OperatorTok{$}\NormalTok{delta)}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
    \DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
\NormalTok{facst2}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/plots-9} \end{center}

Let's now create our first ARMA models (equivalent to ARIMA with 2nd
argument = 0). We'll begin with the first hypothesis:
\(\mathbb{P}(+) = \mathbb{P}(-).\) Modelling this with an AR(1), we
have:

\[Sign_{t+1} = \alpha + \beta Sign_t + \varepsilon, \hspace{2em} \varepsilon \sim wn(0, \sigma^2)\]
In R, we'll use the package \emph{forecast} to construct this model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR1sign <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{sign, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(AR1sign)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: df$sign 
## ARIMA(1,0,0) with non-zero mean 
## 
## Coefficients:
##          ar1    mean
##       0.0278  0.5165
## s.e.  0.0215  0.0111
## 
## sigma^2 estimated as 0.2498:  log likelihood=-1560.63
## AIC=3127.26   AICc=3127.27   BIC=3144.28
## 
## Training set error measures:
##                        ME      RMSE       MAE  MPE MAPE     MASE          ACF1
## Training set 2.157119e-05 0.4995356 0.4990724 -Inf  Inf 1.027755 -0.0006313563
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(AR1sign)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/AR(1)-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(AR1sign}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/AR(1)-2} \end{center}

With the results of the summary, we can now apply a hypothesis test for
our first
question.\footnote{Testing $\beta$ is equivalent to testing $\gamma$.}

\[ H_0: \beta = 0\] \[H_1: \beta \neq 0\]

\(\dfrac{\hat{ar_1} - ar_1}{s.e.(ar_1)}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR1sign}\OperatorTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(AR1sign}\OperatorTok{$}\NormalTok{var.coef[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      ar1 
## 1.287942
\end{verbatim}

The second hypothesis in the problem refers to the delta of the
variation: \[\mathbb{E}(\Delta | + ) \neq \mathbb{E}(\Delta | - ).\]

\[\Delta_{t+1} = \alpha + \beta Sign_t + \varepsilon, \hspace{2em} \varepsilon \sim wn(0, \sigma^2).\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lmsignt <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(delta }\OperatorTok{~}\StringTok{ }\KeywordTok{lag}\NormalTok{(df}\OperatorTok{$}\NormalTok{sign, }\DataTypeTok{k =} \DecValTok{1}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ df)}
\KeywordTok{summary}\NormalTok{(lmsignt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = delta ~ lag(df$sign, k = 1), data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.3285 -0.4706 -0.0060  0.4655  7.9442 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(>|t|)
## (Intercept)          0.01293    0.02811   0.460    0.646
## lag(df$sign, k = 1)  0.05802    0.03911   1.484    0.138
## 
## Residual standard error: 0.9066 on 2150 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.001023,   Adjusted R-squared:  0.000558 
## F-statistic: 2.201 on 1 and 2150 DF,  p-value: 0.1381
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{lag}\NormalTok{(df}\OperatorTok{$}\NormalTok{sign, }\DataTypeTok{k =} \DecValTok{1}\NormalTok{), }\DataTypeTok{y =}\NormalTok{ delta)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Use of `df$sign` is discouraged. Use `sign` instead.
\end{verbatim}

\begin{verbatim}
## `geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{verbatim}
## Warning: Removed 1 rows containing non-finite values (stat_smooth).
\end{verbatim}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/lm signt-1} \end{center}

\[\Delta_{t+1} = \alpha + \beta_1 \Delta_t + \beta_2Sign_t + \varepsilon, \hspace{2em} \varepsilon \sim wn(0, \sigma^2)\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR1delta <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\DataTypeTok{xreg =} \KeywordTok{lag}\NormalTok{(df}\OperatorTok{$}\NormalTok{sign, }
    \DataTypeTok{k =} \DecValTok{1}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(AR1delta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: df$delta 
## Regression with ARIMA(1,0,0) errors 
## 
## Coefficients:
##          ar1  intercept    xreg
##       0.0321     0.0343  0.0166
## s.e.  0.0306     0.0351  0.0556
## 
## sigma^2 estimated as 0.8219:  log likelihood=-2840.96
## AIC=5689.93   AICc=5689.94   BIC=5712.62
## 
## Training set error measures:
##                        ME      RMSE      MAE MPE MAPE      MASE         ACF1
## Training set 1.147232e-05 0.9059341 0.643417 NaN  Inf 0.7252668 0.0003998294
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR1delta}\OperatorTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(AR1delta}\OperatorTok{$}\NormalTok{var.coef[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     ar1 
## 1.04747
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(AR1delta)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/delta 1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(AR1delta}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/delta 1-2} \end{center}

The last hypothesis in the problem refers to the variance:
\[\mathbb{E}(\Delta_{t+1}^2 | \Delta_t).\]

\[\Delta^2_{t+1} = \alpha + \beta \Delta^2_t + \varepsilon, \hspace{2em} \varepsilon \sim wn(0, \sigma^2)\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR1var <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{((df}\OperatorTok{$}\NormalTok{delta)}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(AR1var)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: (df$delta)^2 
## ARIMA(1,0,0) with non-zero mean 
## 
## Coefficients:
##          ar1    mean
##       0.1701  0.8238
## s.e.  0.0212  0.0582
## 
## sigma^2 estimated as 5.026:  log likelihood=-4792.09
## AIC=9590.17   AICc=9590.18   BIC=9607.2
## 
## Training set error measures:
##                         ME     RMSE       MAE  MPE MAPE      MASE        ACF1
## Training set -6.299133e-05 2.240784 0.9197335 -Inf  Inf 0.8595655 -0.01320252
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR1var}\OperatorTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(AR1var}\OperatorTok{$}\NormalTok{var.coef[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      ar1 
## 8.011092
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(AR1var)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/var1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(AR1var}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/var1-2} \end{center}

Now, let's run \emph{auto.arima}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aadelta <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{stepwise =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(aadelta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: df$delta 
## ARIMA(1,0,1) with non-zero mean 
## 
## Coefficients:
##           ar1     ma1    mean
##       -0.7138  0.7506  0.0433
## s.e.   0.1486  0.1399  0.0199
## 
## sigma^2 estimated as 0.8208:  log likelihood=-2840.87
## AIC=5689.74   AICc=5689.76   BIC=5712.44
## 
## Training set error measures:
##                        ME      RMSE       MAE MPE MAPE    MASE        ACF1
## Training set 2.610156e-05 0.9053381 0.6434199 NaN  Inf 0.72527 0.003320553
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(aadelta)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(aadelta}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aasign <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{sign, }\DataTypeTok{stepwise =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(aasign)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: df$sign 
## ARIMA(0,0,0) with non-zero mean 
## 
## Coefficients:
##         mean
##       0.5165
## s.e.  0.0108
## 
## sigma^2 estimated as 0.2498:  log likelihood=-1561.46
## AIC=3126.91   AICc=3126.92   BIC=3138.26
## 
## Training set error measures:
##                         ME      RMSE       MAE  MPE MAPE     MASE       ACF1
## Training set -2.382602e-13 0.4997281 0.4994563 -Inf  Inf 1.028545 0.02773919
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(aasign)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in plot.Arima(aasign): No roots to plot
\end{verbatim}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(aasign}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aavar <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{((df}\OperatorTok{$}\NormalTok{delta)}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DataTypeTok{stepwise =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(aavar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: (df$delta)^2 
## ARIMA(0,1,4) 
## 
## Coefficients:
##           ma1      ma2     ma3      ma4
##       -0.8662  -0.0671  0.0228  -0.0641
## s.e.   0.0215   0.0284  0.0280   0.0214
## 
## sigma^2 estimated as 4.892:  log likelihood=-4761.22
## AIC=9532.45   AICc=9532.48   BIC=9560.82
## 
## Training set error measures:
##                       ME     RMSE       MAE  MPE MAPE      MASE         ACF1
## Training set -0.02170361 2.209213 0.8904046 -Inf  Inf 0.8321553 0.0001189823
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(aavar)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-5} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(aasign}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-6} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aardelta <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{max.q =} \DecValTok{0}\NormalTok{, }\DataTypeTok{stepwise =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(aardelta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: df$delta 
## ARIMA(1,0,0) with non-zero mean 
## 
## Coefficients:
##          ar1    mean
##       0.0382  0.0433
## s.e.  0.0215  0.0203
## 
## sigma^2 estimated as 0.8215:  log likelihood=-2842.26
## AIC=5690.52   AICc=5690.53   BIC=5707.54
## 
## Training set error measures:
##                         ME      RMSE       MAE MPE MAPE      MASE         ACF1
## Training set -1.464203e-05 0.9059233 0.6434299 NaN  Inf 0.7252813 0.0004805619
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(aardelta)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-7} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(aardelta}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-8} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aarsign <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{sign, }\DataTypeTok{max.q =} \DecValTok{0}\NormalTok{, }\DataTypeTok{stepwise =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(aarsign)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: df$sign 
## ARIMA(0,0,0) with non-zero mean 
## 
## Coefficients:
##         mean
##       0.5165
## s.e.  0.0108
## 
## sigma^2 estimated as 0.2498:  log likelihood=-1561.46
## AIC=3126.91   AICc=3126.92   BIC=3138.26
## 
## Training set error measures:
##                         ME      RMSE       MAE  MPE MAPE     MASE       ACF1
## Training set -2.382602e-13 0.4997281 0.4994563 -Inf  Inf 1.028545 0.02773919
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(aarsign)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in plot.Arima(aarsign): No roots to plot
\end{verbatim}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-9} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(aarsign}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-10} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aarvar <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{((df}\OperatorTok{$}\NormalTok{delta)}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DataTypeTok{max.q =} \DecValTok{0}\NormalTok{, }\DataTypeTok{stepwise =}\NormalTok{ F)}
\KeywordTok{summary}\NormalTok{(aarvar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: (df$delta)^2 
## ARIMA(5,1,0) 
## 
## Coefficients:
##           ar1      ar2      ar3      ar4      ar5
##       -0.7420  -0.5845  -0.4042  -0.2873  -0.1687
## s.e.   0.0213   0.0259   0.0274   0.0258   0.0212
## 
## sigma^2 estimated as 5.465:  log likelihood=-4878.95
## AIC=9769.89   AICc=9769.93   BIC=9803.94
## 
## Training set error measures:
##                         ME     RMSE       MAE  MPE MAPE      MASE       ACF1
## Training set -3.234587e-05 2.334504 0.9170278 -Inf  Inf 0.8570369 -0.0239684
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(aarvar)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-11} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tsdisplay}\NormalTok{(aarvar}\OperatorTok{$}\NormalTok{residuals)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P1_files/figure-latex/auto arima-12} \end{center}

\[ \Delta_{t+1} = c + \beta \Delta_t + \varepsilon\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AR1_}\DecValTok{2}\NormalTok{ <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{delta, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}

\KeywordTok{summary}\NormalTok{(AR1_}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Series: df$delta 
## ARIMA(1,0,0) with non-zero mean 
## 
## Coefficients:
##          ar1    mean
##       0.0382  0.0433
## s.e.  0.0215  0.0203
## 
## sigma^2 estimated as 0.8215:  log likelihood=-2842.26
## AIC=5690.52   AICc=5690.53   BIC=5707.54
## 
## Training set error measures:
##                         ME      RMSE       MAE MPE MAPE      MASE         ACF1
## Training set -1.464203e-05 0.9059233 0.6434299 NaN  Inf 0.7252813 0.0004805619
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{confint}\NormalTok{(AR1_}\DecValTok{2}\NormalTok{, }\DataTypeTok{level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                  2.5 %     97.5 %
## ar1       -0.003988796 0.08042284
## intercept  0.003511958 0.08308440
\end{verbatim}


\chapter{Problem 2: Estimating ARMA models}

\{HAMILTON 5.1-5.6\}


\chapter{Problem 3: Identification of ARMA models}

In this problem, we'll be tackling the issue of \emph{identification} of
an ARMA model. Namely, we will employ the \emph{Box-Jenkins} model
selection strategy, based upon the concept of \emph{parsimony}.

The principle of \emph{parsimony} is inspired on the trade-off between
\emph{fit}, i.e., \(R^2\), and \emph{degrees of freedom}. ``Box and
Jenkins argue that parsimonious models produce better forecasts than
overparametrized models''. (p.~76)

The Box-Jenkins strategy is divided in three main stages:

\begin{itemize}
	\item Identification;
	\item Estimation;
	\item Diagnostic checking.
\end{itemize}

These estimations depend upon two essential conditions (discussed in
earlier problems and lectures): \emph{stationarity} and
\emph{invertibility}. Stationarity, as we have discussed earlier, is
necessary to effectively \emph{employ econometric methods} and to infer
characteristics of a population through a given sample. Enders also
points out that t-statistics and Q-statistics are based upon the
assumption that the data are stationary (p.~77). This implies a
condition on the \emph{AR} process of an ARMA model (roots of
characteristic polynomial outside of unity circle).

Furthermore, the model shall be \emph{invertible} -- i.e., if it can be
represented by a finite or convergent AR model. This implies a condition
on the \emph{MA} process -- i.e., if it can be written as an
AR(\(\infty\)).

We're going to check these conditions intuitively by plotting the ACFs
and PACFs of the time series:

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(df)}
		
		\NormalTok{pplot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ t, }\DataTypeTok{y =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Time series plot"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{pplot}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/plots-1} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{acf_ts <-}\StringTok{ }\KeywordTok{Acf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/plots-2} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{acf_test_values <-}\StringTok{ }\NormalTok{acf_ts}\OperatorTok{$}\NormalTok{acf}\OperatorTok{/}\KeywordTok{sd}\NormalTok{(acf_ts}\OperatorTok{$}\NormalTok{acf)}
		
		\KeywordTok{head}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(acf_test_values))}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##   acf_test_values
	## 1       6.5814152
	## 2       3.9180772
	## 3      -0.6619326
	## 4      -2.6109255
	## 5      -1.4713722
	## 6       0.6589976
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facst <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{faclt <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
		
		\NormalTok{facpst <-}\StringTok{ }\KeywordTok{ggPacf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{100}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facplt <-}\StringTok{ }\KeywordTok{ggPacf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facst}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/plots-3} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{faclt}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/plots-4} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facpst}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/plots-5} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facplt}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/plots-6} \end{center}

Aside from usual methods, we'll employ the following criteria:

\begin{itemize}
	\item Akaike Information Criterion (AIC). 
	$$ AIC = T * ln(SSR) + 2n $$
	\item  Schwartz Bayesian Criterion (SBC).
	$$ SBC = T * ln(SSR) + n * ln(T) $$
\end{itemize}

\(n\) denotes the number of parameters estimated (an useful metric given
the importance of the degrees of freedom). \(T\) denotes the number of
\emph{usable} observations. Note that, when comparing different models,
it is important to \emph{fix} \(T\) to ensure that the \(AIC\) and
\(SBC\) values are comparable and are capturing only variations in the
actual model and not the effect of changing T.

The objective with these criteria is to \emph{minimize} their values.
``As the fit of the model improves, the AIC and SBC will approach
-\(\infty\).'' (p.~70) \(AIC\) and \(SBC\) have different advantages and
drawbacks: while the former is biased toward overparametrization and
more powerful in small samples, \(SBC\) is consistent and has superior
large sample properties. If both metrics point to the same model, we
should be fairly confident that it is, indeed, the correct
specification.

It is also important to apply hypothesis tests to the estimates of the
population parameters \(\mu, \sigma^2\) and \(\rho_s\) --
\(\bar{y}, \hat{sigma}^2, r_s\), respectively. Worthy of note here is
\(r_s\), which presents the following distributions under the null that
\(y_t\) is stationary with \(\varepsilon_t \sim \mathcal{N}\):
\[ Var(r_s) = T^{-1} \hspace{2em} for \, s = 1\]
\[ Var(r_s) = T^{-1}(1 + 2\sum_{j=1}^{s-1} r_j^2) \hspace{2em} for \, s > 1\]

The Q-statistic is also introduced by Enders in this chapter. It is used
to test whether a group of autocorrelations is significantly different
from zero. \[Q = T\sum_{k=1}^s r_k^2\] Under the null of
\(r_k = 0 \forall k\), Q is asymptotically \(\chi^2\) with s degrees of
freedom. ``Certainly, a white-noise process (in which all
autocorrelations should be zero) would have a Q value of zero''. (p.~68)

An alternative form for \(Q\) is presented by Ljung and Box (1978):
\[ Q = T(T+2) \sum_{k=1}^s \dfrac{r_k^2}{(T-k)} \]

Furthermore, it is also important to check whether the residuals of the
model are actually \emph{white noise}. This can be done via the
Q-statistic, which \emph{should not result in the rejection of the
	null}. If that is not the case, the model specified is not the best one
available, as there's still a relevant underlying variable (\(y\) or
\(\varepsilon\)).

Let's now perform the \emph{estimation stage}. This shall be done via
the function \emph{auto.arima} from the package \emph{forecast}.

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{aa_model <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{num.cores =} \DecValTok{24}\NormalTok{, }\DataTypeTok{max.d =} \DecValTok{0}\NormalTok{, }\DataTypeTok{max.D =} \DecValTok{0}\NormalTok{, }
		\DataTypeTok{stepwise =}\NormalTok{ F)}
		
		\KeywordTok{summary}\NormalTok{(aa_model)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Series: df$value 
	## ARIMA(2,0,1) with non-zero mean 
	## 
	## Coefficients:
	##          ar1      ar2    ma1    mean
	##       0.7524  -0.5545  0.797  3.5305
	## s.e.  0.0818   0.0813  0.064  0.3485
	## 
	## sigma^2 estimated as 3.002:  log likelihood=-235.87
	## AIC=481.75   AICc=482.27   BIC=495.68
	## 
	## Training set error measures:
	##                      ME     RMSE      MAE      MPE    MAPE      MASE
	## Training set 0.01363546 1.703559 1.426984 5.237664 82.3373 0.5612557
	##                     ACF1
	## Training set 0.004670695
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{print}\NormalTok{(}\StringTok{"t-values: "}\NormalTok{)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## [1] "t-values: "
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{aa_t <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{4}\NormalTok{)}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{)) \{}
		
		\NormalTok{    aa_t[i] <-}\StringTok{ }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{coef[i]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{var.coef[i, i])}
		
		\NormalTok{\}}
		
		\NormalTok{aa_t <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(aa_t)}
		
		\NormalTok{aa_t}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##        aa_t
	## 1  9.203615
	## 2 -6.822352
	## 3 12.444488
	## 4 10.129782
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{aa_q <-}\StringTok{ }\KeywordTok{Box.test}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{lag =}\NormalTok{ aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])}
		\NormalTok{aa_q}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## 
	##  Box-Pierce test
	## 
	## data:  aa_model$residuals
	## X-squared = 0.078351, df = 3, p-value = 0.9943
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{ggAcf}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/estimation autoarima-1} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{ggPacf}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{center}\includegraphics{Econo2_P3_files/figure-latex/estimation autoarima-2} \end{center}

The results of \emph{auto.arima} imply that the best model is an
ARMA(2,1):
\[ y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \theta_1 \varepsilon_{t-1} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

Furthermore, the Q-statistic \emph{(Box.test)} seems to indicate that
\(\varepsilon_t\) is truly white noise.

Let's now run some different models and compare them against the results
of \emph{auto.arima}. We'll begin with some overspecified model. First,
an ARMA(2,2):
\[ y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma22 <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{2}\NormalTok{))}
		
		\KeywordTok{summary}\NormalTok{(arma22)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Series: df$value 
	## ARIMA(2,0,2) with non-zero mean 
	## 
	## Coefficients:
	##          ar1      ar2     ma1     ma2    mean
	##       0.7417  -0.5502  0.8113  0.0149  3.5311
	## s.e.  0.1442   0.0950  0.1692  0.1631  0.3514
	## 
	## sigma^2 estimated as 3.028:  log likelihood=-235.87
	## AIC=483.74   AICc=484.48   BIC=500.46
	## 
	## Training set error measures:
	##                      ME     RMSE      MAE      MPE     MAPE      MASE
	## Training set 0.01360342 1.703508 1.426237 4.791703 81.74486 0.5609619
	##                     ACF1
	## Training set 0.001161589
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma22_t <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{)}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)) \{}
		
		\NormalTok{    arma22_t[i] <-}\StringTok{ }\NormalTok{arma22}\OperatorTok{$}\NormalTok{coef[i]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(arma22}\OperatorTok{$}\NormalTok{var.coef[i, i])}
		
		\NormalTok{\}}
		
		\NormalTok{arma22_t <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(arma22_t)}
		
		\NormalTok{arma22_t}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##      arma22_t
	## 1  5.14206433
	## 2 -5.78853038
	## 3  4.79577309
	## 4  0.09134106
	## 5 10.04968297
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma22_q <-}\StringTok{ }\KeywordTok{Box.test}\NormalTok{(arma22}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{lag =}\NormalTok{ arma22}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{arma22}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])}
		\NormalTok{arma22_q}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## 
	##  Box-Pierce test
	## 
	## data:  arma22$residuals
	## X-squared = 0.26458, df = 4, p-value = 0.992
\end{verbatim}

The t-value of \(ma2\) is not able to reject the null hypothesis.
Furthermore, the Q-statistic \emph{(Box.test)} seems to indicate that
\(\varepsilon_t\) is truly white noise.

Now, an ARMA(3,1):
\[ y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \Phi_3 y_{t-3} + \theta_1 \varepsilon_{t-1} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma31 <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
		
		\KeywordTok{summary}\NormalTok{(arma31)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Series: df$value 
	## ARIMA(3,0,1) with non-zero mean 
	## 
	## Coefficients:
	##          ar1     ar2     ar3     ma1    mean
	##       0.7610  -0.565  0.0112  0.7923  3.5312
	## s.e.  0.1215   0.137  0.1174  0.0825  0.3516
	## 
	## sigma^2 estimated as 3.028:  log likelihood=-235.87
	## AIC=483.74   AICc=484.48   BIC=500.46
	## 
	## Training set error measures:
	##                      ME     RMSE      MAE      MPE     MAPE      MASE
	## Training set 0.01359734 1.703504 1.426202 4.779284 81.71699 0.5609482
	##                      ACF1
	## Training set 0.0008885573
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma31_t <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{)}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)) \{}
		
		\NormalTok{    arma31_t[i] <-}\StringTok{ }\NormalTok{arma31}\OperatorTok{$}\NormalTok{coef[i]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(arma31}\OperatorTok{$}\NormalTok{var.coef[i, i])}
		
		\NormalTok{\}}
		
		\NormalTok{arma31_t <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(arma31_t)}
		
		\NormalTok{arma31_t}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##      arma31_t
	## 1  6.26539086
	## 2 -4.12346904
	## 3  0.09501299
	## 4  9.59788435
	## 5 10.04172094
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma31_q <-}\StringTok{ }\KeywordTok{Box.test}\NormalTok{(arma31}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{lag =}\NormalTok{ arma31}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{arma31}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])}
		\NormalTok{arma31_q}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## 
	##  Box-Pierce test
	## 
	## data:  arma31$residuals
	## X-squared = 0.25911, df = 4, p-value = 0.9923
\end{verbatim}

The t-value of \(ar3\) is not able to reject the null hypothesis.
Furthermore, the Q-statistic \emph{(Box.test)} seems to indicate that
\(\varepsilon_t\) is truly white noise.

Now, let's try some \emph{underspecified models}. Beginning with an
ARMA(2,0):
\[ y_t = c + \Phi_1 y_{t-1} + \Phi_2 y_{t-2} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma20 <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{))}
		
		\KeywordTok{summary}\NormalTok{(arma20)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Series: df$value 
	## ARIMA(2,0,0) with non-zero mean 
	## 
	## Coefficients:
	##          ar1      ar2    mean
	##       1.0226  -0.7153  3.5194
	## s.e.  0.0634   0.0635  0.2694
	## 
	## sigma^2 estimated as 4.24:  log likelihood=-256.37
	## AIC=520.74   AICc=521.09   BIC=531.89
	## 
	## Training set error measures:
	##                      ME     RMSE      MAE      MPE     MAPE      MASE      ACF1
	## Training set 0.01106086 2.033314 1.630805 73.16585 128.4039 0.6414218 0.2915253
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma20_t <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{3}\NormalTok{)}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)) \{}
		
		\NormalTok{    arma20_t[i] <-}\StringTok{ }\NormalTok{arma20}\OperatorTok{$}\NormalTok{coef[i]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(arma20}\OperatorTok{$}\NormalTok{var.coef[i, i])}
		
		\NormalTok{\}}
		
		\NormalTok{arma20_t <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(arma20_t)}
		
		\NormalTok{arma20_t}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##    arma20_t
	## 1  16.12226
	## 2 -11.26848
	## 3  13.06561
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma20_q <-}\StringTok{ }\KeywordTok{Box.test}\NormalTok{(arma20}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{lag =}\NormalTok{ arma20}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{arma20}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])}
		\NormalTok{arma20_q}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## 
	##  Box-Pierce test
	## 
	## data:  arma20$residuals
	## X-squared = 13.728, df = 2, p-value = 0.001045
\end{verbatim}

The Q-statistic indicates that there is an ommited variable -- namely,
\(\varepsilon_{t-1}\) that we have just excluded from the model.

Now, an ARMA(1,1):
\[ y_t = c + \Phi_1 y_{t-1} +  + \theta_1 \varepsilon_{t-1} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma11 <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
		
		\KeywordTok{summary}\NormalTok{(arma11)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Series: df$value 
	## ARIMA(1,0,1) with non-zero mean 
	## 
	## Coefficients:
	##          ar1     ma1    mean
	##       0.4476  0.9244  3.6027
	## s.e.  0.0831  0.0323  0.6234
	## 
	## sigma^2 estimated as 4.026:  log likelihood=-253.74
	## AIC=515.48   AICc=515.82   BIC=526.63
	## 
	## Training set error measures:
	##                       ME     RMSE      MAE      MPE     MAPE      MASE
	## Training set 0.006283661 1.981184 1.621537 51.07775 142.3988 0.6377767
	##                   ACF1
	## Training set 0.2028683
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma11_t <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{3}\NormalTok{)}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)) \{}
		
		\NormalTok{    arma11_t[i] <-}\StringTok{ }\NormalTok{arma11}\OperatorTok{$}\NormalTok{coef[i]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(arma11}\OperatorTok{$}\NormalTok{var.coef[i, i])}
		
		\NormalTok{\}}
		
		\NormalTok{arma11_t <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(arma11_t)}
		
		\NormalTok{arma11_t}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##    arma11_t
	## 1  5.387948
	## 2 28.624391
	## 3  5.779126
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{arma11_q <-}\StringTok{ }\KeywordTok{Box.test}\NormalTok{(arma11}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{lag =}\NormalTok{ arma11}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{arma11}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])}
		\NormalTok{arma11_q}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## 
	##  Box-Pierce test
	## 
	## data:  arma11$residuals
	## X-squared = 12.668, df = 2, p-value = 0.001775
\end{verbatim}

Again, the Q-statistic indicates that there is an ommited variable --
namely, \(y_{t-1}\) that we have just excluded from the model.

Finally, let's compare the \(AIC\) and \(BIC\) values for all these
models.

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{criteria <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{5}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
		
		\NormalTok{aa_criteria <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"ARMA(2,1)*"}\NormalTok{, aa_model}\OperatorTok{$}\NormalTok{aic, aa_model}\OperatorTok{$}\NormalTok{bic)}
		
		\KeywordTok{names}\NormalTok{(aa_criteria) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"BIC"}\NormalTok{)}
		
		\NormalTok{arma22_criteria <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"ARMA(2,2)"}\NormalTok{, arma22}\OperatorTok{$}\NormalTok{aic, arma22}\OperatorTok{$}\NormalTok{bic)}
		
		\KeywordTok{names}\NormalTok{(arma22_criteria) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"BIC"}\NormalTok{)}
		
		\NormalTok{arma31_criteria <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"ARMA(3,1)"}\NormalTok{, arma31}\OperatorTok{$}\NormalTok{aic, arma31}\OperatorTok{$}\NormalTok{bic)}
		
		\KeywordTok{names}\NormalTok{(arma31_criteria) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"BIC"}\NormalTok{)}
		
		\NormalTok{arma20_criteria <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"ARMA(2,0)"}\NormalTok{, arma20}\OperatorTok{$}\NormalTok{aic, arma20}\OperatorTok{$}\NormalTok{bic)}
		
		\KeywordTok{names}\NormalTok{(arma20_criteria) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"BIC"}\NormalTok{)}
		
		\NormalTok{arma11_criteria <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"ARMA(1,1)"}\NormalTok{, arma11}\OperatorTok{$}\NormalTok{aic, arma11}\OperatorTok{$}\NormalTok{bic)}
		
		\KeywordTok{names}\NormalTok{(arma11_criteria) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"BIC"}\NormalTok{)}
		
		
		\NormalTok{criteria <-}\StringTok{ }\KeywordTok{rbind.data.frame}\NormalTok{(aa_criteria, arma22_criteria, arma31_criteria, }
		\NormalTok{    arma20_criteria, arma11_criteria)}
		
		\NormalTok{criteria}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##        Model      AIC      BIC
	## 1 ARMA(2,1)* 481.7460 495.6834
	## 2  ARMA(2,2) 483.7376 500.4625
	## 3  ARMA(3,1) 483.7369 500.4619
	## 4  ARMA(2,0) 520.7381 531.8880
	## 5  ARMA(1,1) 515.4753 526.6253
\end{verbatim}

As we can clearly see, the model chosen by \emph{auto.arima} is the
optimal choice according both to AIC and BIC.




\chapter{Forecasting}

Suppose that you observe a time series up to period T, $Y_1, Y_2, ..., Y_T,$ and would like to forecast its value in $T+1$, or, more generically, up to a time horizon $h \geq 1$: $Y_{T+1}, ..., Y_{T+h}$.

Let $g(Y_1, ..., Y_T)$ be a general predictor of $Y_{T+1}$ built with information up to period $T$. We can measure its utility with its \textit{mean squared error (MSE)}:
$$ MSE[g(Y_1, ..., Y_T)] := \mathbb{E}[(Y_{T+1} - g(Y_1, ..., Y_T))^2] $$

We know that the conditional mean is the predictor that minimizes MSE:
$$\mathbb{E}\left(Y_{T+1} \mid Y_{T}, \ldots, Y_{1}\right)=\operatorname{argmin}_{g} \mathbb{E}\left[\left(Y_{t+1}-g\left(Y_{1}, Y_{2}, \ldots, Y_{T}\right)\right)^{2}\right]$$

Unfortunately, we usually don't have the functional form of $\mathbb{E}\left(Y_{T+1} \mid Y_{T}, \ldots, Y_{1}\right)$, and postulate its best linear form:
$$\pi\left(Y_{T+1} \mid Y_{T}, \ldots, Y_{1}\right)=\alpha+\beta_{0} Y_{T}+\beta_{1} Y_{T-1}+\ldots \beta_{T-1} Y_{1}$$

Denote the best linear predictor with information up to T as $Y_{T+1|T} := \pi(Y_{T+1}|Y_t,..., Y_1)$, and its estimated version as $\hat{Y}_{T+1|T} := \hat{\pi}(Y_{T+1}|Y_t,..., Y_1) = \hat{\alpha}+\hat{\beta}_{0} Y_{T}+\hat{\beta}_{1} Y_{T-1}+\ldots \hat{\beta}_{T-1} Y_{1}$.

\section{Forecasting with an AR(1) model}

Remember from Definition \ref{ar1-def} that an AR(1) model has the following form:
$$ Y_t = c + \phi Y_{t-1} + \varepsilon_{t}$$

\subsection{Forecast}

Given that $\mathbb{E}(\varepsilon_{t+1}|Y_T) = 0$ (as $\epsilon \sim wn(0, \sigma^2)$), we have: 

$$
Y_{T+1 \mid T}:=\pi\left(Y_{T+1} \mid Y_{T}, \ldots, Y_{1}\right)=c+\phi Y_{T}
$$
For $T+2$, we have:
$$
\begin{aligned}
	Y_{T+2 \mid T} &:=\pi\left(Y_{T+2} \mid Y_{T}, \ldots, Y_{1}\right) \\
	&=\pi\left(\pi\left(Y_{T+2} \mid Y_{T+1}, Y_{T}, \ldots, Y_{1}\right) \mid Y_{T}, \ldots, Y_{1}\right) \\
	&=\pi\left(c+\phi Y_{T+1} \mid Y_{T}, \ldots, Y_{1}\right) \\
	&=c+\phi\left(c+\phi Y_{T}\right)=(1+\phi) c+\phi^{2} Y_{T}
\end{aligned}
$$
The clear pattern here yields, more generally:
$$
Y_{T+h \mid T}=\left(1+\phi+\phi^{2}+\cdots+\phi^{h-1}\right) c+\phi^{h} Y_{T}
$$

\subsection{Forecast error}

It is also easy to verify that the \textit{forecast error} for $h=1$ is given by:
$$ u_{T+1|T} := Y_{T+1} - Y_{T+1|T} = Y_{T+1} - c - \phi Y_T = \varepsilon_{T+1} $$
 
For $h=2$, we have:
$$
\begin{aligned}
 u_{T+2 \mid T} &:=Y_{T+2}-Y_{T+2 \mid T} \\
 &=c+\phi Y_{T+1}+\epsilon_{T+2}-\left(c+\phi Y_{T+1 \mid T}\right) \\
 &=\phi\left(Y_{T+1}-Y_{T+1 \mid T}\right)+\epsilon_{T+2} \\
 &=\phi u_{T+1 \mid T}+\epsilon_{T+2}=\phi \epsilon_{T+1}+\epsilon_{T+2}
\end{aligned}
$$
More generally, for a given time horizon $h$:
$$
u_{T+h \mid T}=\left(\epsilon_{T+h}+\phi \epsilon_{T+h-1}+\cdots+\phi^{h-1} \epsilon_{T+1}\right)$$

\subsection{Mean reversion}

Note that, as $h$ increases, the prediction \textit{reverts to the unconditional mean:}

$$\lim _{h \rightarrow \infty} Y_{T+h \mid T}=c \lim _{h \rightarrow \infty} \sum_{i=0}^{h-1} \phi^{i}+Y_{T} \lim _{h \rightarrow \infty} \phi^{h}=\frac{c}{1-\phi}=: \mu$$

The variance of the forecast error is given by:
$$ Var(u_{T+h|T}) = (1 + \phi^{2} + \phi^4 + ... + \phi^{2(h-1)})\sigma^2 $$

Taking the limit as $h \to \infty$, the variance of the forecast error \textit{also approaches the unconditional variance of the process:}
$$\lim _{h \rightarrow \infty} \mathbb{V}\left(u_{T+h \mid T}\right)=\sigma^{2} \lim _{h \rightarrow \infty} \sum_{i=0}^{h-1} \phi^{2 i}=\frac{\sigma^{2}}{1-\phi^{2}}=: \gamma_{0}$$

\section{Forecasting with an AR(p) model}

\subsection{Forecast}

The same procedure can be applied to an AR(p). For $h = 1$: 
$$
Y_{T+1 \mid T}=\pi\left(Y_{T+1} \mid Y_{T}, \ldots, Y_{1}\right)=c+\phi_{1} Y_{T}+\cdots+\phi_{p} Y_{T-p+1}
$$
For a given $h$, proceed recursively:
$$\begin{aligned}
	Y_{T+2 \mid T} &=c+\phi_{1} Y_{T+1 \mid T}+\phi_{2} Y_{T}+\phi_{3} Y_{T-1}+\cdots+\phi_{p} Y_{T+2-p} \\
	Y_{T+3 \mid T} &=c+\phi_{1} Y_{T+2 \mid T}+\phi_{2} Y_{T+1 \mid T}+\phi_{3} Y_{T}+\cdots+\phi_{p} Y_{T+3-p} \\
	Y_{T+4 \mid T} &=c+\phi_{1} Y_{T+3 \mid T}+\phi_{2} Y_{T+2 \mid T}+\phi_{3} Y_{T+1 \mid T}+\cdots+\phi_{p} Y_{T+4-p} \\
	& \vdots \\
	Y_{T+h \mid T} &=c+\phi_{1} Y_{T+h-1 \mid T}+\phi_{2} Y_{T+h-2 \mid T}+\cdots+\phi_{p} Y_{T+h-p \mid T}
\end{aligned}$$

\subsection{Forecast error}

For $h=1$, the forecast error is given by:
$$ u_{T+1|T} := Y_{T+1} - Y_{T+1|T} = \varepsilon_{T+1}$$

As $h$ increases:
$$\begin{aligned}
	u_{T+2 \mid T} &=\phi_{1} u_{T+1 \mid T}+\epsilon_{T+2} \\
	u_{T+3 \mid T} &=\phi_{1} u_{T+2 \mid T}+\phi_{2} u_{T+1 \mid T}+\epsilon_{T+3} \\
	& \vdots \\
	u_{T+h \mid T} &=\phi_{1} u_{T+h-1 \mid T}+\cdots+\phi_{h-1} u_{T+1 \mid T}+\epsilon_{T+h},
\end{aligned}$$
where $\phi_{h-1} = 0$ for $h-1>p$.

\section{Forecasting with a MA(1) model}

Remember from \ref{ma1-def} that a MA(1) model is given by
$$ Y_t = c + \theta \varepsilon_{t-1} + \varepsilon_{t} $$
$$ Y_{T+1|T} := \pi(Y_{T+1} | Y_T,..., Y_1) $$

We have seen that, if the MA(1) is invertible, we can write it as an AR($\infty$). This would mean, however, that the forecast errors would depend on \textit{all past values} -- notwithstanding the decreasing dependence, due to ergodicity. Given a fixed $\varepsilon_{0}$, we can reconstruct the entire error series: 
$$\begin{aligned}
	\epsilon_{1} &=Y_{1}-c-\theta \epsilon_{0} \\
	\epsilon_{2} &=Y_{2}-c-\theta \epsilon_{1} \\
	& \vdots \\
	\epsilon_{T} &=Y_{T}-c-\theta \epsilon_{T-1}
\end{aligned}$$

If we do not know $\varepsilon_{0}$, we can approximate it by its (known!) mean, $\tilde{\varepsilon}_0 = 0$. If $T$ is large enough and $|\theta| < 1$, this is a good approximation. That is the case because for large $T$, the influence of the assumption $\tilde{\varepsilon}_0 = 0$ is geometrically diminished over time, given $|\theta| < 1$. 


\subsection{Forecast}

We are now able to construct the forecast using the estimated $\tilde{\varepsilon}_T$.
$$ Y_{T+1|T} := c + \theta \tilde{\varepsilon}_T $$

Iteratively for larger horizons:
$$Y_{T+2 \mid T}=c+\theta \widetilde{\epsilon}_{T+1}=c+\theta\left(Y_{T+1 \mid T}-c-\theta \widetilde{\epsilon}_{T}\right)=c$$

Note that \textit{the MA(1) process is not predictable for $h > 1$.} The forecast immediately mean reverts at $h>1$.

\subsection{Forecast error}

The forecast error, assuming $\tilde{\varepsilon}_T \approx \varepsilon_{T}$, is given by: 
$$ u_{T+1|T} := Y_{T+1} - Y_{T+1|T} = \varepsilon_{T+1}$$

This means that the variance of the forecast error is $\sigma^2$. For larger horizons, the variance converges to the unconditional variance: 
$$ u_{T+h|T} := Y_{T+h} - Y_{T+h|T} = \varepsilon_{T+h} + \theta \varepsilon_{T+h-1}, \forall h>1$$

Thus, $ Var(u_{T+h|T}) = (1+\theta)^2 \sigma^2 = \gamma_0, \forall h > 1$.

\section{Forecasting with a MA(q) model}

We can proceed iteratively for the MA(q) model.

$$\begin{aligned}
	Y_{T+1 \mid T} &=c+\theta_{1} \widetilde{\epsilon}_{T}+\cdots+\theta_{q} \widetilde{\epsilon}_{T+1-q} \\
	Y_{T+2 \mid T} &=c+\theta_{2} \widetilde{\epsilon}_{T}+\cdots+\theta_{q} \widetilde{\epsilon}_{T+2-q} \\
	Y_{T+3 \mid T} &=c+\theta_{3} \widetilde{\epsilon}_{T}+\cdots+\theta_{q} \widetilde{\epsilon}_{T+3-q} \\
	& \vdots \\
	Y_{T+q \mid T} &=c+\theta_{q} \widetilde{\epsilon}_{T}
\end{aligned}$$

Note that, after $q$ periods, the prediction is the unconditional mean $c$.

\section{Forecast with an ARMA(p,q) model}

Let's combine the procedures presented for AR(p) and MA(q). For $h=1$:
$$Y_{T+1 \mid T}=c+\phi_{1} Y_{T}+\cdots+\phi_{p} Y_{T-p+1}+\theta_{1} \widetilde{\varepsilon}_{T}+\cdots+\theta_{q} \widetilde{\varepsilon}_{T+1-q},$$
where $\{\widetilde{\varepsilon}_{T}, ..., \widetilde{\varepsilon}_{T+1-q}\}$ were obtained from the reconstruction process explained in the MA(q) case. However, here we need, aside from the first $q$ innovation values, also the \textit{last $p$ observations before the first one}. That is the case because of the expression for $\widetilde{\varepsilon}_{1}$:

$$\widetilde{\epsilon}_{1}=Y_{1}-c-\phi_{1} Y_{0}-\cdots-\phi_{p} Y_{p-1}-\theta_{1} \widetilde{\epsilon}_{0}-\theta_{2} \widetilde{\epsilon}_{-1}-\cdots-\theta_{q} \widetilde{\epsilon}_{1-q}$$

Alternatively, we can start reconstructing the residuals from $p+1$. 

\subsection{Forecast}

From this, we can obtain the forecasts for any horizon $h \geq 1$, given $h>p>q$: 
$$\begin{array}{l}
	Y_{T+1 \mid T}=c+\phi_{1} Y_{T}+\cdots+\phi_{p} Y_{T+1-p}+\theta_{1} \widetilde{\epsilon}_{T}+\cdots+\theta_{q} \widetilde{\epsilon}_{T+1-q} \\
	Y_{T+2 \mid T}=c+\phi_{1} Y_{T+1 \mid T}+\cdots+\phi_{p} Y_{T+2-p}+\theta_{2} \widetilde{\epsilon}_{T}+\cdots+\theta_{q} \widetilde{\epsilon}_{T+2-q} \\
	Y_{T+3 \mid T}=c+\phi_{1} Y_{T+2 \mid T}+\cdots+\phi_{p} Y_{T+3-p}+\theta_{3} \widetilde{\epsilon}_{T}+\cdots+\theta_{q} \widetilde{\epsilon}_{T+3-q}
\end{array}$$

$ \hspace{13em} \vdots$

$$\begin{aligned}
	Y_{T+q \mid T} &=c+\phi_{1} Y_{T+q-1 \mid T}+\cdots+\phi_{p} Y_{T+q-p}+\theta_{q} \widetilde{\epsilon}_{T} \\
	Y_{T+q+1 \mid T} &=c+\phi_{1} Y_{T+q \mid T}+\cdots+\phi_{p} Y_{T+q+1-p} \\
	& \vdots \\
	Y_{T+h \mid T} &=c+\phi_{1} Y_{T+h-1 \mid T}+\cdots+\phi_{p} Y_{T+h-p \mid T}
\end{aligned}$$

\section{Confidence intervals for forecasts}

Given a forecast $\hat{Y}_{T+h|T}$, we'd like to have a \textit{confidence interval} for it. In practice, forecasting involves two types of errors:
\begin{itemize}
	\item \textbf{Estimation error:} $Y_{T+h|T} - \hat{Y}_{T+h|T}$. This is due to the estimation of the ARMA model, as we are always working with a sample.
	\item \textbf{Forecast error:} $u_{T+h|T} = Y_{T+h} - Y_{T+h|T}$. This is the portion of the error that would exist event if we knew the true population parameters.
\end{itemize}

From this, we have the \textit{aggregate error:}
$$ Y_{T+h} - \hat{Y}_{T+h|T} = (Y_{T+h} - {Y}_{T+h|T}) + (Y_{T+h|T} - \hat{Y}_{T+h|T}) $$

\subsection{Normally distributed errors}

A first way to tackle the issue of confidence intervals for forecasts is to \textit{assume normality}: $\varepsilon_{t} \sim \mathcal{N}(0, \sigma^2)$. 

In that case, we know that $Y_{T+h}, \hat{Y}_{T+h|T}$ will also be normally distributed, as the first term consists of a combination of past errors and past values of Y, and the second term is asymptotically normal given the properties of ARMA estimators. This means that the confidence interval is known to us, for $\alpha = 5\%$:
$$\left[\widehat{Y}_{T+h \mid T} \pm 1,96 \sqrt{\mathbb{V}\left(u_{T+h \mid T}\right)}\right]$$

We clearly do not observe $Var(u_{T+h|T})$, but we know its functional form and, thus, can consistently estimate it. For an AR(1), for example: 
$$\widehat{\mathbb{V}}\left(u_{T+h \mid T}\right)=\widehat{\sigma}^{2} \sum_{i=0}^{h-1} \widehat{\phi}^{2 i}$$

Note that, even if we assume that the innovation terms are normally distributed, we \textit{still need an asymptotic argument} to guarantee that $\widehat{Y}_{T+h \mid T} \stackrel{p}{\longrightarrow} Y_{T+h \mid T} \text { and } \widehat{\mathbb{V}}\left(u_{T+h \mid T}\right) \stackrel{p}{\longrightarrow} \mathbb{V}\left(u_{T+h \mid T}\right)$.

\subsection{Bootstrapping}

What if the errors \textit{are not normally distributed?} This makes the issue a lot more complicated. Note that, even if we apply the CLT to argue that $Y_{T+h|T} - \hat{Y}_{T+h|T} \sim \mathcal{N}(\cdot)$, what makes us believe that $(Y_{T+h} - {Y}_{T+h|T})$ is also normally distributed? In the last subsection, this condition was valid because of the distribution of the errors. There's no asymptotical argument to be made. 

In practice, we don't know the distribution of the error terms. In fact, they are \textit{rarely normally distributed!} A procedure to construct confidence intervals in this setting is called \textit{bootstrap.} For ARMA models, it consists in generating simulated samples from the actual sample and repeat the estimations for each simulated sample. This yields an arbitrarily large number of estimates \textit{from the same data}. From this, we can use its \textit{empirical distribution} to find the confidence interval.

Bootstrapping has a number of advantages over the traditional procedures for obtaining a confidence interval. In many cases, the convergence is faster than the usual $1/\sqrt{T}$, and it frequently performs better in small sample environments. It can also be applied in settings in which asymptotic theory is very intricate. In practice, we almost always use some sort of bootstrap method. 

The essential idea behind bootstrapping is to assume that $\varepsilon_{t}$ is \textit{iid} with a cdf $F$. Suppose that you have $T$ residuals, $\hat{\varepsilon}_t$. Its empirical distribution is simply: 
$$ \hat{F}_T(v) = \dfrac{\{ no. \, residuals \leq v \}}{T}, \hspace{1em} v \in \mathbb{R}$$

The Law of Large Numbers guarantees that, as $T \to \infty$, 
$$ \hat{F}_T(v) \to_{a.s.} F(v) $$
This means that the empirical distribution of the residuals becomes arbitrarily close to the real distribution as $T$ grows.

\subsubsection{Procedure}

Bootstrapping for an ARMA(p,q) model involves the following steps:

\begin{enumerate}
	\item \begin{itemize}
		\item Estimate ARMA(p,q)
		$$ Y_t = c + \sum_{j=1}^p \phi_j Y_{t-j} + \sum_{j=1}^q \theta_j \varepsilon_{t-j} + \varepsilon_t$$
		
		\item Calculate the residuals of the regression:
		$$ \hat{\varepsilon}_t := Y_t - (\hat{c} + \sum_{j=1}^p \hat{\phi}_j Y_{t-j} + \sum_{j=1}^q \hat{\theta}_j \varepsilon_{t-j})$$
		
		\item If the residuals do not have mean 0, create the centered residuals:
		$$ \tilde{\varepsilon}_t = \hat{\varepsilon}_t - \frac{1}{t} \sum_{t=1}^T \hat{\varepsilon}_t$$
	\end{itemize}
	
	\item \begin{itemize}
		\item Select at random, with replacement, a sample with $T + m$ elements, $m >> 0$:
		$$\{\varepsilon^*_1, ..., \varepsilon^*_{T+m}\}$$
		\item Create a series $\{Y^*_t\}_{t=1}^{T+m}$:
		$$ Y_t^* = Y_t, 1 \leq t \leq max(p,q) $$
		$$ Y_t^* = \hat{c} + \sum_{j=1}^p \hat{\phi}_j Y_{t-j} + \sum_{j=1}^q \hat{\theta}_j \varepsilon_{t-j}^* + \varepsilon_t^*, max(p,q) < t \leq T + m $$
	\end{itemize}
	\item \begin{itemize}
		\item Using the simulated sample $\{Y^*_t\}_{t=1}^{T+m}$, create a forecast for $h >0$ periods using the estimated coefficients \textit{obtained with the real sample}.
		\item This yields a vector of dimension $h$ containing the forecasts in the form:
		$$(\hat{Y}^*_{T+1}, ..., \hat{Y}^*_{T+h})$$ 
		\item Repeat steps 2 and 3 for $S$ times. Create a matrix with the results.
		\item This yields a S x h matrix where each row is equal to the aforementioned vector.
	\end{itemize}
\end{enumerate}

\chapter{Problem 4: Cross-validation and bootstrap}

In this problem, we'll be tackling the issue of \emph{forecasting} of an
ARMA model. The problem is split in two parts: (i)
\emph{cross-validation}; and (ii) \emph{bootstrapping}.

\section{Identification and estimation}

First, let's identify the best model for our time series.

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(df)}
		
		\NormalTok{pplot <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ df, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ t, }\DataTypeTok{y =}\NormalTok{ value)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Time series plot"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{pplot}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/plots-1} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{acf_ts <-}\StringTok{ }\KeywordTok{Acf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/plots-2} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{acf_test_values <-}\StringTok{ }\NormalTok{acf_ts}\OperatorTok{$}\NormalTok{acf}\OperatorTok{/}\KeywordTok{sd}\NormalTok{(acf_ts}\OperatorTok{$}\NormalTok{acf)}
		
		\KeywordTok{head}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(acf_test_values))}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##   acf_test_values
	## 1        6.176432
	## 2        2.515951
	## 3        3.335438
	## 4        1.864909
	## 5        1.112884
	## 6        0.858639
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facst <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{faclt <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
		
		\NormalTok{facpst <-}\StringTok{ }\KeywordTok{ggPacf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{100}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facplt <-}\StringTok{ }\KeywordTok{ggPacf}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{5000}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facst}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/plots-3} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{faclt}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/plots-4} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facpst}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/plots-5} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facplt}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/plots-6} \end{center}

We'll now use the function \emph{auto.arima} from the package
\emph{forecast} to identify and estimate the model.

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{aa_model <-}\StringTok{ }\KeywordTok{auto.arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{num.cores =} \DecValTok{24}\NormalTok{, }\DataTypeTok{max.d =} \DecValTok{0}\NormalTok{, }\DataTypeTok{stepwise =}\NormalTok{ F)}
		
		\KeywordTok{summary}\NormalTok{(aa_model)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Series: df$value 
	## ARIMA(0,0,3) with non-zero mean 
	## 
	## Coefficients:
	##          ma1     ma2     ma3    mean
	##       0.1814  0.6647  0.4001  5.8982
	## s.e.  0.0852  0.0750  0.0949  0.0562
	## 
	## sigma^2 estimated as 0.0667:  log likelihood=-5.42
	## AIC=20.85   AICc=21.49   BIC=33.88
	## 
	## Training set error measures:
	##                        ME      RMSE       MAE        MPE     MAPE      MASE
	## Training set -0.002315954 0.2530428 0.2131067 -0.2268814 3.612855 0.7314965
	##                    ACF1
	## Training set 0.03868106
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{print}\NormalTok{(}\StringTok{"t-values: "}\NormalTok{)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## [1] "t-values: "
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{aa_t <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{)) \{}
		
		\NormalTok{    aa_t[i] <-}\StringTok{ }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{coef[i]}\OperatorTok{/}\KeywordTok{sqrt}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{var.coef[i, i])}
		
		\NormalTok{\}}
		
		\NormalTok{aa_t <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(aa_t)}
		
		\NormalTok{aa_t}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##         aa_t
	## 1   2.128691
	## 2   8.861580
	## 3   4.216481
	## 4 105.004537
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{aa_q <-}\StringTok{ }\KeywordTok{Box.test}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{lag =}\NormalTok{ aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])}
		\NormalTok{aa_q}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## 
	##  Box-Pierce test
	## 
	## data:  aa_model$residuals
	## X-squared = 0.35002, df = 3, p-value = 0.9504
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{criteria <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{, }\DataTypeTok{ncol =} \DecValTok{3}\NormalTok{)}
		
		\NormalTok{aa_criteria <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\StringTok{"MA(3)*"}\NormalTok{, aa_model}\OperatorTok{$}\NormalTok{aic, aa_model}\OperatorTok{$}\NormalTok{bic)}
		
		\KeywordTok{names}\NormalTok{(aa_criteria) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Model"}\NormalTok{, }\StringTok{"AIC"}\NormalTok{, }\StringTok{"BIC"}\NormalTok{)}
		
		\NormalTok{aa_criteria}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##    Model      AIC      BIC
	## 1 MA(3)* 20.84963 33.87549
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{fac_e <-}\StringTok{ }\KeywordTok{ggAcf}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
		
		\NormalTok{facp_e <-}\StringTok{ }\KeywordTok{ggPacf}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{type =} \StringTok{"correlation"}\NormalTok{, }\DataTypeTok{lag.max =} \DecValTok{20}\NormalTok{, }
		\DataTypeTok{plot =}\NormalTok{ T) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Warning: Ignoring unknown parameters: type
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{fac_e}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/estimation autoarima-1} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{facp_e}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/estimation autoarima-2} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{mean}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## [1] -0.002315954
\end{verbatim}

The results of \emph{auto.arima} imply that the best model is an
ARMA(0,3) -- i.e., a MA(3):
\[ y_t = c +  + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2}+ \theta_3 \varepsilon_{t-3} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

Furthermore, the Q-statistic \emph{(Box.test)} seems to indicate that
\(\varepsilon_t\) is truly white noise.

\section{Cross-validation}

Let's now cross-validate or model. This will now be done manually;
afterwards, an automatized version from \emph{fpp} shall be presented.

Let \(h := 5\); \(frac = 0.2\). \(T\) is the size of our sample; \(k\)
is the \emph{training} database. The remainder shall be used for testing
purposes.

As we have discovered previously, \emph{auto.arima} yields a
\textbf{MA(3)} model. It will now be used.

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{h <-}\StringTok{ }\DecValTok{5}
		
		\NormalTok{frac <-}\StringTok{ }\FloatTok{0.2}
		
		\NormalTok{T <-}\StringTok{ }\KeywordTok{length}\NormalTok{(df}\OperatorTok{$}\NormalTok{value)}
		
		\NormalTok{k <-}\StringTok{ }\KeywordTok{floor}\NormalTok{((}\DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{frac) }\OperatorTok{*}\StringTok{ }\NormalTok{T)}
		
		\CommentTok{# Estimating MA(3) with k = 80}
		\NormalTok{fit <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value[}\DecValTok{1}\OperatorTok{:}\NormalTok{k], }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
		
		\CommentTok{# Generating predictions from the model}
		\NormalTok{pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, }\DataTypeTok{n.ahead =}\NormalTok{ h)}
		
		\CommentTok{# Calculating errors between the predicted values of the}
		\CommentTok{# model and the actual values of the testing database}
		
		\NormalTok{e <-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{value[(k }\OperatorTok{+}\StringTok{ }\NormalTok{h)] }\OperatorTok{-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{pred[h]}
		
		\NormalTok{e}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## [1] -0.1951299
\end{verbatim}

Let's now update our training database iteratively with a for loop.

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{e <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{100}\NormalTok{)}
		
		\CommentTok{# Updating the model}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ k}\OperatorTok{:}\NormalTok{(T }\OperatorTok{-}\StringTok{ }\NormalTok{h)) \{}
		
		\NormalTok{    fit <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value[}\DecValTok{1}\OperatorTok{:}\NormalTok{i], }\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{3}\NormalTok{))}
		
		\NormalTok{    pred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit, }\DataTypeTok{n.ahead =}\NormalTok{ h)}
		
		\NormalTok{    e[i, }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{value[(i }\OperatorTok{+}\StringTok{ }\NormalTok{h)] }\OperatorTok{-}\StringTok{ }\NormalTok{pred}\OperatorTok{$}\NormalTok{pred[h]}
		
		\NormalTok{\}}
	\end{Highlighting}
\end{Shaded}

With the matrix \emph{e} in hands, we can now calculate MSE:

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{mse <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(e}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DataTypeTok{na.rm =}\NormalTok{ T)}
	\end{Highlighting}
\end{Shaded}

This procedure can now be used to compare other models against the model
from \emph{auto.arima}.

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{max_p <-}\StringTok{ }\DecValTok{5}
		
		\NormalTok{max_q <-}\StringTok{ }\DecValTok{5}
		
		\NormalTok{e <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{100}\NormalTok{, }\DataTypeTok{ncol =}\NormalTok{ (max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(max_q }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{))}
		
		\NormalTok{pred <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, (max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(max_q }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{))}
		
		\NormalTok{fit <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, (max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(max_q }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{))}
		
		\CommentTok{# Updating the model}
		\ControlFlowTok{for}\NormalTok{ (u }\ControlFlowTok{in} \DecValTok{0}\OperatorTok{:}\NormalTok{max_q) \{}
		
		\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{0}\OperatorTok{:}\NormalTok{max_p) \{}
		
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ k}\OperatorTok{:}\NormalTok{(T }\OperatorTok{-}\StringTok{ }\NormalTok{h)) \{}
		
		\NormalTok{            fit[[(((max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{j) }\OperatorTok{+}\StringTok{ }\NormalTok{u }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)]] <-}\StringTok{ }\KeywordTok{Arima}\NormalTok{(df}\OperatorTok{$}\NormalTok{value[}\DecValTok{1}\OperatorTok{:}\NormalTok{i], }
		\DataTypeTok{order =} \KeywordTok{c}\NormalTok{(j, }\DecValTok{0}\NormalTok{, u))}
		
		\CommentTok{# fit <- append(fit, Arima(df$value[1:i], order = c(j,0,u)))}
		
		\CommentTok{# pred <- append(pred, predict(fit[[(j+u)]], n.ahead = h))}
		
		\NormalTok{            pred[[(((max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{j) }\OperatorTok{+}\StringTok{ }\NormalTok{u }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)]] <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(fit[[(((max_p }\OperatorTok{+}\StringTok{ }
		\StringTok{                }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{j) }\OperatorTok{+}\StringTok{ }\NormalTok{u }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)]], }\DataTypeTok{n.ahead =}\NormalTok{ h)}
		
		\NormalTok{            e[i, (((max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{j) }\OperatorTok{+}\StringTok{ }\NormalTok{u }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)] <-}\StringTok{ }\NormalTok{df}\OperatorTok{$}\NormalTok{value[(i }\OperatorTok{+}\StringTok{ }
		\StringTok{                }\NormalTok{h)] }\OperatorTok{-}\StringTok{ }\NormalTok{pred[[(((max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{j) }\OperatorTok{+}\StringTok{ }\NormalTok{u }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)]]}\OperatorTok{$}\NormalTok{pred[h]}
		
		\NormalTok{        \}}
		
		\NormalTok{    \}}
		
		\NormalTok{\}}
		
		\NormalTok{mse <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ ((max_p }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{(max_q }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)), }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{)}
		
		
		
		
		\NormalTok{mse <-}\StringTok{ }\KeywordTok{colMeans}\NormalTok{(e}\OperatorTok{^}\DecValTok{2}\NormalTok{, }\DataTypeTok{na.rm =}\NormalTok{ T)}
		
		\NormalTok{mse}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##  [1] 0.1357466 0.1354001 0.1368083 0.1374243 0.1376508 0.1441940 0.1347115
	##  [8] 0.1269779 0.1347789 0.1373465 0.1398588 0.1436175 0.1313779 0.1315448
	## [15] 0.1435805 0.1355649 0.1421277 0.1335153 0.1316765 0.1333955 0.1400838
	## [22] 0.1427467 0.1473227 0.1347447 0.1320856 0.1333025 0.1354734 0.1341742
	## [29] 0.1380676 0.1357880 0.1346228 0.1382810 0.1319484 0.1308446 0.1382417
	## [36] 0.1327046
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{optimal_index <-}\StringTok{ }\KeywordTok{which.min}\NormalTok{(mse)}
		
		\NormalTok{cv_model <-}\StringTok{ }\NormalTok{fit[[optimal_index]]}
		
		\KeywordTok{summary}\NormalTok{(cv_model)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	## Series: df$value[1:i] 
	## ARIMA(1,0,1) with non-zero mean 
	## 
	## Coefficients:
	##          ar1      ma1    mean
	##       0.8253  -0.4888  5.9125
	## s.e.  0.0814   0.1118  0.0815
	## 
	## sigma^2 estimated as 0.08209:  log likelihood=-14.72
	## AIC=37.43   AICc=37.88   BIC=47.65
	## 
	## Training set error measures:
	##                        ME      RMSE       MAE        MPE     MAPE      MASE
	## Training set -0.002725722 0.2819456 0.2228183 -0.2756862 3.787114 0.7600473
	##                    ACF1
	## Training set -0.1279409
\end{verbatim}

The cross-validation method constructed above yielded an ARMA(1,1):
\[ y_t = c + \phi_1 y_{t-1} +  + \theta_1 \varepsilon_{t-1} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{cv_fc <-}\StringTok{ }\KeywordTok{forecast}\NormalTok{(cv_model, }\DataTypeTok{h =}\NormalTok{ h)}
		
		\KeywordTok{autoplot}\NormalTok{(cv_fc)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/forecast cv plot-1} \end{center}

\section{Bootstrapping}

Now, let's proceed to \emph{bootstrapping}. It envolves the following
steps:

\begin{enumerate}
	\item \begin{itemize}
		\item Estimate ARMA(p,q)
		$$ Y_t = c + \sum_{j=1}^p \phi_j Y_{t-j} + \sum_{j=1}^q \theta_j \varepsilon_{t-j} + \varepsilon_t$$
		
		\item Calculate the residuals of the regression:
		$$ \hat{\varepsilon}_t := Y_t - (\hat{c} + \sum_{j=1}^p \hat{\phi}_j Y_{t-j} + \sum_{j=1}^q \hat{\theta}_j \varepsilon_{t-j})$$
		
		\item If the residuals do not have mean 0, create the centered residuals:
		$$ \tilde{\varepsilon}_t = \hat{\varepsilon}_t - \frac{1}{t} \sum_{t=1}^T \hat{\varepsilon}_t$$
	\end{itemize}
	
	\item \begin{itemize}
		\item Select at random, with replacement, a sample with $T + m$ elements, $m >> 0$:
		$$\{\varepsilon^*_1, ..., \varepsilon^*_{T+m}\}$$
		\item Create a series $\{Y^*_t\}_{t=1}^{T+m}$:
		$$ Y_t^* = Y_t, 1 \leq t \leq max(p,q) $$
		$$ Y_t^* = \hat{c} + \sum_{j=1}^p \hat{\phi}_j Y_{t-j} + \sum_{j=1}^q \hat{\theta}_j \varepsilon_{t-j}^* + \varepsilon_t^*, max(p,q) < t \leq T + m $$
	\end{itemize}
	\item \begin{itemize}
		\item Using the simulated sample $\{Y^*_t\}_{t=1}^{T+m}$, create a forecast for $h >0$ periods using the estimated coefficients \textit{obtained with the real sample}.
		\item This yields a vector of dimension $h$ containing the forecasts in the form:
		$$(\hat{Y}^*_{T+1}, ..., \hat{Y}^*_{T+h})$$ 
		\item Repeat steps 2 and 3 for $S$ times. Create a matrix with the results.
		\item This yields a S x h matrix where each row is equal to the aforementioned vector.
	\end{itemize}
\end{enumerate}

We'll use, again, the optimal model from \emph{auto.arima}, MA(3):

\[ y_t = c +  + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2}+ \theta_3 \varepsilon_{t-3} + \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2)\]

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{S <-}\StringTok{ }\DecValTok{1000}
		
		\NormalTok{m <-}\StringTok{ }\DecValTok{100}
		
		
		
		\NormalTok{optimal_p <-}\StringTok{ }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{]}
		
		\NormalTok{optimal_q <-}\StringTok{ }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{]}
		
		\NormalTok{e_sample <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ S, }\DataTypeTok{ncol =}\NormalTok{ (}\KeywordTok{length}\NormalTok{(df}\OperatorTok{$}\NormalTok{value) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{m)))}
		
		\NormalTok{y_star <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ S, }\DataTypeTok{ncol =}\NormalTok{ (}\KeywordTok{length}\NormalTok{(df}\OperatorTok{$}\NormalTok{value) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{m }\OperatorTok{+}\StringTok{ }\KeywordTok{max}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{], aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{]))))}
		
		\NormalTok{arima_star <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ S, }\DataTypeTok{ncol =}\NormalTok{ (}\KeywordTok{length}\NormalTok{(df}\OperatorTok{$}\NormalTok{value) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{m }\OperatorTok{+}\StringTok{ }\KeywordTok{max}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{], aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{]))))}
		
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{S) \{}
		
		\NormalTok{    e_sample[i] <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{residuals, }\DataTypeTok{replace =}\NormalTok{ T, }\DataTypeTok{size =}\NormalTok{ (}\KeywordTok{length}\NormalTok{(df}\OperatorTok{$}\NormalTok{value) }\OperatorTok{+}\StringTok{ }
		\StringTok{        }\NormalTok{m))}
		
		\NormalTok{\}}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{S) \{}
		
		\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in}\NormalTok{ ((aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{] }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}\OperatorTok{:}\NormalTok{(}\KeywordTok{length}\NormalTok{(df}\OperatorTok{$}\NormalTok{value) }\OperatorTok{+}\StringTok{ }
		\StringTok{        }\NormalTok{m))) \{}
		
		\NormalTok{        arima_star[i, j] <-}\StringTok{ }\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{coef[}\DecValTok{4}\NormalTok{] }\OperatorTok{+}\StringTok{ }\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{coef[}\DecValTok{1}\NormalTok{] }\OperatorTok{*}\StringTok{ }
		\StringTok{            }\NormalTok{e_sample[i, j }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{] }\OperatorTok{*}\StringTok{ }\NormalTok{e_sample[i, }
		\NormalTok{            j }\OperatorTok{-}\StringTok{ }\DecValTok{2}\NormalTok{]) }\OperatorTok{+}\StringTok{ }\NormalTok{(aa_model}\OperatorTok{$}\NormalTok{coef[}\DecValTok{3}\NormalTok{] }\OperatorTok{*}\StringTok{ }\NormalTok{e_sample[i, j }\OperatorTok{-}\StringTok{ }\DecValTok{3}\NormalTok{]) }\OperatorTok{+}\StringTok{ }
		\StringTok{            }\NormalTok{e_sample[i, j])}
		
		\NormalTok{    \}}
		
		\NormalTok{\}}
		
		
		\NormalTok{y_fixed <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ S, }\DataTypeTok{ncol =}\NormalTok{ (aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{1}\NormalTok{] }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\NormalTok{aa_model}\OperatorTok{$}\NormalTok{arma[}\DecValTok{2}\NormalTok{])))}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{S) \{}
		\NormalTok{    y_fixed[i, }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(df}\OperatorTok{$}\NormalTok{value[}\DecValTok{1}\NormalTok{])}
		\NormalTok{    y_fixed[i, }\DecValTok{2}\NormalTok{] <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(df}\OperatorTok{$}\NormalTok{value[}\DecValTok{2}\NormalTok{])}
		\NormalTok{    y_fixed[i, }\DecValTok{3}\NormalTok{] <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(df}\OperatorTok{$}\NormalTok{value[}\DecValTok{3}\NormalTok{])}
		\NormalTok{\}}
		
		
		\NormalTok{y_star <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(y_fixed, arima_star[, }\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)])}
		
		\NormalTok{y_m <-}\StringTok{ }\NormalTok{y_star[, }\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{100}\NormalTok{)]}
		
		\NormalTok{y_m <-}\StringTok{ }\NormalTok{y_m[, }\OperatorTok{-}\NormalTok{(}\DecValTok{101}\OperatorTok{:}\DecValTok{103}\NormalTok{)]}
		
		\NormalTok{y_mt <-}\StringTok{ }\KeywordTok{t}\NormalTok{(y_m)}
		
		\NormalTok{y_matrix <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(y_m)}
	\end{Highlighting}
\end{Shaded}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{fc_list <-}\StringTok{ }\KeywordTok{vector}\NormalTok{(}\StringTok{"list"}\NormalTok{, S)}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{S) \{}
		
		\NormalTok{    fc_list[[i]] <-}\StringTok{ }\KeywordTok{forecast}\NormalTok{(}\KeywordTok{ts}\NormalTok{(y_matrix[i, ]), }\DataTypeTok{model =}\NormalTok{ aa_model, }
		\DataTypeTok{h =} \DecValTok{5}\NormalTok{)}
		
		\NormalTok{\}}
		
		\NormalTok{fc_list[[}\DecValTok{1}\NormalTok{]]}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
	## 101       5.880900 5.549926 6.211875 5.374719 6.387082
	## 102       5.869038 5.532663 6.205413 5.354597 6.383479
	## 103       5.841494 5.439567 6.243421 5.226800 6.456189
	## 104       5.898184 5.475000 6.321368 5.250980 6.545387
	## 105       5.898184 5.475000 6.321368 5.250980 6.545387
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{fc_mean <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ S, }\DataTypeTok{ncol =} \DecValTok{5}\NormalTok{))}
		
		\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{S) \{}
		
		\NormalTok{    fc_mean[i, ] <-}\StringTok{ }\NormalTok{fc_list[[i]]}\OperatorTok{$}\NormalTok{mean}
		
		\NormalTok{\}}
	\end{Highlighting}
\end{Shaded}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{head}\NormalTok{(fc_mean)}
	\end{Highlighting}
\end{Shaded}

\begin{verbatim}
	##         X1       X2       X3       X4       X5
	## 1 5.880900 5.869038 5.841494 5.898184 5.898184
	## 2 5.734428 5.543250 5.786803 5.898184 5.898184
	## 3 5.728049 5.722659 5.832225 5.898184 5.898184
	## 4 5.844103 5.943213 5.958997 5.898184 5.898184
	## 5 5.550780 5.518844 5.732659 5.898184 5.898184
	## 6 5.742853 5.863462 5.848949 5.898184 5.898184
\end{verbatim}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{hist_x1 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fc_mean, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X1)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{40}\NormalTok{) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{hist_x1}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-1} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{qq_x1 <-}\StringTok{ }\KeywordTok{qqnorm}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X1)}
		\KeywordTok{qqline}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X1)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-2} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{hist_x2 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fc_mean, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X2)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{40}\NormalTok{) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{hist_x2}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-3} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{qq_x2 <-}\StringTok{ }\KeywordTok{qqnorm}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X2)}
		\KeywordTok{qqline}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X2)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-4} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{hist_x3 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fc_mean, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X3)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{40}\NormalTok{) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{hist_x3}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-5} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{qq_x3 <-}\StringTok{ }\KeywordTok{qqnorm}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X3)}
		\KeywordTok{qqline}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X3)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-6} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{hist_x4 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fc_mean, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X4)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{40}\NormalTok{) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{hist_x4}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-7} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{qq_x4 <-}\StringTok{ }\KeywordTok{qqnorm}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X4)}
		\KeywordTok{qqline}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X4)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-8} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{hist_x5 <-}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fc_mean, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X5)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{100}\NormalTok{) }\OperatorTok{+}\StringTok{ }
		\StringTok{    }\KeywordTok{theme_few}\NormalTok{()}
		\NormalTok{hist_x5}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-9} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{qq_x5 <-}\StringTok{ }\KeywordTok{qqnorm}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X5)}
		\KeywordTok{qqline}\NormalTok{(fc_mean}\OperatorTok{$}\NormalTok{X5)}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/mean ic-10} \end{center}

The results show that, from \(h \geq 4\), the predicted value is the
mean of the series.

Now, some forecasting plots:

\begin{Shaded}
	\begin{Highlighting}[]
		\NormalTok{fc <-}\StringTok{ }\KeywordTok{forecast}\NormalTok{(df}\OperatorTok{$}\NormalTok{value, }\DataTypeTok{model =}\NormalTok{ aa_model, }\DataTypeTok{h =}\NormalTok{ h)}
		
		\KeywordTok{autoplot}\NormalTok{(fc) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/forecast arima plots-1} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{autoplot}\NormalTok{(fc_list[[}\DecValTok{1}\NormalTok{]]) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/forecast arima plots-2} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{autoplot}\NormalTok{(fc_list[[}\DecValTok{66}\NormalTok{]]) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/forecast arima plots-3} \end{center}

\begin{Shaded}
	\begin{Highlighting}[]
		\KeywordTok{autoplot}\NormalTok{(fc_list[[}\DecValTok{796}\NormalTok{]]) }\OperatorTok{+}\StringTok{ }\KeywordTok{theme_few}\NormalTok{()}
	\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{Econo2_P4_files/figure-latex/forecast arima plots-4} \end{center}























































\end{document}
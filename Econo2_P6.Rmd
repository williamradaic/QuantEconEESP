---
title: "Econometrics II - Problem 6"
author: "William Radaic Peron"
date: \today
output:
  pdf_document: 
    keep_tex: yes
  html_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = FALSE)

library(ggplot2)
library(forecast)
library(AER)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
library(ggthemes)
library(gridExtra)
library(readxl)
library(ggthemes)
library(mdscore)
library(readxl)
library(fpp)
library(sidrar) 
library(mFilter)
library(detrendr)
library(dynlm)
library(ggfortify)
library(tseries)
# install.packages(c('dynlm','ggfortify','tseries'), repos='http://cran.rstudio.com/')
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
library(ggplot2)
library(forecast)
library(AER)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
library(ggthemes)
library(gridExtra)
library(gtsummary)
library(readxl)
library(ggthemes)
library(mdscore)
library(readxl)
library(fpp)


library(ggplot2)
library(forecast)
library(AER)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
library(ggthemes)
library(gridExtra)
library(gtsummary)
library(readxl)
library(ggthemes)
library(mdscore)

library(formatR)
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

```

\section{Part 2: Unit roots}

As has been discussed during the lecture, when the population model is a *random walk*:
$$ Y_t = 1*Y_{t-1} + \varepsilon_t, \hspace{1em} \varepsilon \sim wn(0, \sigma^2), $$

it happens that the series *is not ergodic* (nor is it stationary). Therefore, the usual asymptotic properties *do not hold*, as all innovations have *permanent effects*. 
$$ Y_t = c + \delta t + \sum_{i=1}^t \varepsilon_i $$

The random walk, defined above, is an example of the more general class of *unit root processes*:
$$ Y_t = c + \delta t + u_t,$$

$u_t$ has an ARMA(p,q) representation:

$$ \Phi_p (L) u_t = \Theta_q (L) \varepsilon_t, \hspace{1em} \varepsilon \sim wn(0, \sigma^2)$$

Suppose that one of the roots of $\Theta_p (L)$ is equal to 1:

$$ \Theta_p (L) = (1 - [1]L)(1 - \lambda_2 L)...(1 - \lambda_p L)$$

$$ (1 - L)u_t = (1 - \lambda_2 L)^{-1} ... (1 - \lambda_p L)^{-1} \Theta_q (L) \varepsilon_t =: \Psi(L) \varepsilon_t $$

We can now rewrite this as:
$$ (1- L)Y_t = (1-L)c + (1-L)\delta t + (1-L)u_t $$

$$ (1 - L)Y_t = \delta + \Psi (L) \varepsilon_t $$

Defining $\Delta Y_t$, we have: 
$$ \Delta Y_t := (1 - L)Y_t = Y_t - Y_{t-1} = \delta + \Psi (L) \varepsilon_t $$

With this concept, we can define ARIMA(p,d,q) processes:
$$ \Phi_p (L) (1-L)^d Y_t = c + \Theta_q (L) \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2) $$

\subsection{Hypothesis testing}

It turns out that testing for unit root presence presents some challenges. Under the null ($a_1 = 1$), its distribution *is not standard* and does not present the usual asymptotic properties. We can circumvent this issue with the use of numeric methods, such as the *Monte Carlo simulation*. It will now be employed, following Dickey and Fuller (1979).

First, some notation: 

We begin with a simple model:
$$ Y_t = a_1 y_{t-1} + \varepsilon_t $$
$$ \Delta Y_t = \gamma y_{t-1} + \varepsilon_t, \hspace{1em} \gamma := a_1 - 1 $$ 
Dickey and Fuller constructed the following regression equations:
\begin{equation}
 \Delta y_t = \gamma y_{t-1} + \varepsilon_t
\end{equation}

\begin{equation}
\Delta y_t = a_0 + \gamma y_{t-1} + \varepsilon_t
\end{equation}

\begin{equation}
\Delta y_t = a_0 + \gamma y_{t-1} + a_2t + \varepsilon_t
\end{equation}

(1) has no intercept and represents a simple random walk. (2) includes an intercept. (3) also adds a deterministic time trend. 

Note that the critical values of the t-statistics are *different* between these regressions. This will now be shown with our Monte Carlo simulation.

For equation (1):


``` {r monte carlo 1, tidy = FALSE}

set.seed(76345)

# length of ts
T <- 100

# Loops
S <- 10000

e <- rnorm(T, 0, 1)

# As y_{t+1} = y_t + \varepsilon_t, we can write this as an MA(\infty) model. y_t = \sum \varepsilon_i. This is now done with the cumsum function.

y <- cumsum(e)

y <- vector()

y[1] = e[1]

for (i in (2:T)) {

    y[i] <- y[(i-1)] + e[i]

}

y <- as.ts(y)

autoplot(y) + theme_bw()

# Taking the first difference

y_diff <- diff(y)

# Regression of I(1) model

reg <- dynlm(y_diff ~ 0 + L(y,1)) # no lag (x1 = 0)

reg <-summary(reg)

reg$coefficients[1,3] # t value

# Loop

tau <- vector()

k <- 1

delta <- 0

for (i in 1:S) {
  
  e <- rnorm(T,0,1)
  y = k + delta*seq(1:T) + cumsum(e)
  y <- as.ts(y)
  
  y_diff <- diff(y)
  
  reg <- summary(dynlm(y_diff ~ 1 + L(y, 1)))
  
  tau[i] <- reg$coefficients[2,3]
  
}

tau.df <- data.frame(tau)

ggplot(data = tau.df, aes(x = tau)) + geom_density(color = "blue") + stat_function(fun = dnorm, n = 101, args = c(mean = 0, sd = 1)) + theme_few()


jarque.bera.test(tau) # We reject H0 at the 1% significance level.

tau.ci <- quantile(tau, c(0.01, 0.05, 0.1))

tau.ci


```

For equation (2) -- with an intercept:

```{r monte carlo 2}

# Loop

tau_mu <- vector()

for (i in 1:S) {
  
  e <- rnorm(T,0,1)
  y <- cumsum(e)
  y <- as.ts(y)
  
  y_diff <- diff(y)
  
  reg <- summary(dynlm(y_diff ~ 1 + L(y, 1)))
  
  tau_mu[i] <- reg$coefficients[2,3]
  
}

tau_mu.df <- data.frame(tau_mu)

ggplot(data = tau_mu.df, aes(x = tau_mu)) + geom_density(color = "blue") + stat_function(fun = dnorm, n = 101, args = c(mean = 0, sd = 1)) + theme_few()


jarque.bera.test(tau_mu) # We reject H0 at the 1% significance level.

tau_mu.ci <- quantile(tau_mu, c(0.01, 0.05, 0.1))

tau_mu.ci



```

And finally, for equation (3) -- with an intercept and a deterministic time trend:

```{r monte carlo 3}

# Loop

tau_t <- vector()
time <- c(1:T)

for (i in 1:S) {
  
  e <- rnorm(T,0,1)
  y <- cumsum(e)
  y <- as.ts(y)
  
  y_diff <- diff(y)
  
  reg <- summary(dynlm(y_diff ~ 1 + L(y, 1) + time[-1])) # removed 1 dimension for no. of obs. 
  
  tau_t[i] <- reg$coefficients[2,3]
  
}

tau_t.df <- data.frame(tau_t)

ggplot(data = tau_t.df, aes(x = tau_t)) + geom_density(color = "blue") + stat_function(fun = dnorm, n = 101, args = c(mean = 0, sd = 1)) + theme_few()


jarque.bera.test(tau_t) # We reject H0 at the 1% significance level.

tau_t.ci <- quantile(tau_t, c(0.01, 0.05, 0.1))

tau_t.ci



```

Let's now change the distributions of the errors for equation (3):

```{r monte carlo poisson}

# Loop

tau_t_pois <- vector()
time <- c(1:T)

for (i in 1:S) {
  
  e <- rpois(T,1)
  y <- cumsum(e)
  y <- as.ts(y)
  
  y_diff <- diff(y)
  
  reg <- summary(dynlm(y_diff ~ 1 + L(y, 1) + time[-1])) # removed 1 dimension for no. of obs. 
  
  tau_t_pois[i] <- reg$coefficients[2,3]
  
}

tau_t_pois.df <- data.frame(tau_t_pois)

ggplot(data = tau_t_pois.df, aes(x = tau_t_pois)) + geom_density(color = "blue") + stat_function(fun = dnorm, n = 101, args = c(mean = 0, sd = 1)) + theme_few()


jarque.bera.test(tau_t_pois) # We reject H0 at the 1% significance level.

tau_t_pois.ci <- quantile(tau_t_pois, c(0.01, 0.05, 0.1))

tau_t_pois.ci



```

\section{Part 3: Applying the Dickey-Fuller test for GDP}

Loading the data from the previous problem: 

```{r loading gdp data}


pib <- get_sidra(6612, variable = 9318, category = 90707, period = "all")

pib_limpo <- pib[(pib$`Setores e subsetores (CÃ³digo)` == 90707),]

pib <- pib_limpo

pib_limpo2 <- pib[,c(5,13)]

pib <- pib_limpo2

names(pib)[1] <- "t"
names(pib)[2] <- "v"

pib$t <- as.numeric(pib$t)

names(pib)

head(pib)

tail(pib)

pib <- ts(pib$v)

```


```{r auto arima GDP}


# Choosing the correct model with auto.arima

aa_pib <- auto.arima(pib, stepwise = F)

summary(aa_pib) 


```

*auto.arima* yields an ARIMA(2,1,2) model with a drift parameter:
$$ \Delta y_t = a_0 + a_1 \Delta y_{t-1} + a_2 \Delta y_{t-2} + a_3 \Delta \varepsilon_{t-1} + a_4 \Delta \varepsilon_{t-2} +\varepsilon_t $$

Let's decompose this process. First, we'll perform the Dickey-Fuller test on the GDP ts.

```{r df gdp test}

adf.test(pib)

```

As we have not been able to reject $H_0$, it follows that the ts includes (at least one) unit root. Now, let's find the optimal model for the regression.

```{r optimal model manual}

max_p <- 5

max_q <- 5

max_d <- 2

models1 <- vector("list", (max_p + 1) * (max_q + 1))

models2 <- vector("list", (max_p + 1) * (max_q + 1))

# Updating the model

fit1 <- vector("list", (max_p + 1) * (max_q + 1))

fit2 <- vector("list", (max_p + 1) * (max_q + 1))

model_info1 <- data.frame(matrix(NA, nrow = ((max_p + 1) * (max_q + 1)), ncol = 3))




# Updating the model

for (u in 0:max_q) {

for (j in 0:max_p) { 

  
 fit1[[(((max_p+1)*j)+u + 1)]] <- Arima(pib, order = c(j,1,u))
 
 model_info1[(((max_p+1)*j)+u + 1), 1:2] <- c(fit1[[(((max_p+1)*j)+u + 1)]]$aic, fit1[[(((max_p+1)*j)+u + 1)]]$bic)
   
   

 
}
}

names(model_info1) <- c("AIC", "BIC", "void")



which.min(model_info1$AIC)
which.min(model_info1$BIC)

fit1[[which.min(model_info1$AIC)]]

fit1[[which.min(model_info1$BIC)]]

# For I(2)

model_info2 <- data.frame(matrix(NA, nrow = max_d*((max_p + 1) * (max_q + 1)), ncol = 3))

for (u in 0:max_q) {

for (j in 0:max_p) { 

  
 fit2[[(((max_p+1)*j)+u + 1)]] <- Arima(pib, order = c(j,2,u))
 
 model_info2[(((max_p+1)*j)+u + 1), 1:2] <- c(fit2[[(((max_p+1)*j)+u + 1)]]$aic, fit2[[(((max_p+1)*j)+u + 1)]]$bic)
  
}
}

names(model_info2) <- c("AIC", "BIC", "void")

which.min(model_info2$AIC)
which.min(model_info2$BIC)

fit2[[which.min(model_info2$AIC)]]

fit2[[which.min(model_info2$BIC)]]


```
---
title: "Econometrics II - Problem 6"
author: "William Radaic Peron, Vinícius Marcial"
date: "September 3, 2020"
output: pdf_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy = FALSE)

Sys.setenv(RSTUDIO_PANDOC="C:/Users/William/anaconda3/envs/py_r/Library/Scripts/pandoc")

library(ggplot2)
library(forecast)
library(AER)
library(tidyverse)
library(tidyr)
library(dplyr)
library(corrplot)
library(ggthemes)
library(gridExtra)
library(readxl)
library(ggthemes)
library(mdscore)
library(readxl)
library(fpp)
library(sidrar)
library(mFilter)
library(detrendr)
library(dynlm)
library(ggfortify)
library(tseries)
# install.packages(c('dynlm','ggfortify','tseries'), repos='http://cran.rstudio.com/')

```

\section{Part 1: Spurious regressions}

{{matéria}}



\section{Part 2: Unit roots}

As has been discussed during the lecture, when the population model is a *random walk*:
$$ Y_t = 1*Y_{t-1} + \varepsilon_t, \hspace{1em} \varepsilon \sim wn(0, \sigma^2), $$

it happens that the series *is not ergodic* (nor is it stationary). Therefore, the usual asymptotic properties *do not hold*, as all innovations have *permanent effects*. 
$$ Y_t = c + \delta t + \sum_{i=1}^t \varepsilon_i $$

The random walk, defined above, is an example of the more general class of *unit root processes*:
$$ Y_t = c + \delta t + u_t,$$

$u_t$ has an ARMA(p,q) representation:

$$ \Phi_p (L) u_t = \Theta_q (L) \varepsilon_t, \hspace{1em} \varepsilon \sim wn(0, \sigma^2)$$

Suppose that one of the roots of $\Theta_p (L)$ is equal to 1:

$$ \Theta_p (L) = (1 - [1]L)(1 - \lambda_2 L)...(1 - \lambda_p L)$$

$$ (1 - L)u_t = (1 - \lambda_2 L)^{-1} ... (1 - \lambda_p L)^{-1} \Theta_q (L) \varepsilon_t =: \Psi(L) \varepsilon_t $$

We can now rewrite this as:
$$ (1- L)Y_t = (1-L)c + (1-L)\delta t + (1-L)u_t $$

$$ (1 - L)Y_t = \delta + \Psi (L) \varepsilon_t $$

Defining $\Delta Y_t$, we have: 
$$ \Delta Y_t := (1 - L)Y_t = Y_t - Y_{t-1} = \delta + \Psi (L) \varepsilon_t $$

With this concept, we can define ARIMA(p,d,q) processes:
$$ \Phi_p (L) (1-L)^d Y_t = c + \Theta_q (L) \varepsilon_t, \hspace{1em} \varepsilon_t \sim wn(0, \sigma^2) $$

\subsection{Hypothesis testing}

It turns out that testing for unit root presence presents some challenges. Under the null ($a_1 = 1$), its distribution *is not standard* and does not present the usual asymptotic properties. We can circumvent this issue with the use of numeric methods, such as the *Monte Carlo simulation*. It will now be employed, following Dickey and Fuller (1979).

First, some notation: 

We begin with a simple model:
$$ Y_t = a_1 y_{t-1} + \varepsilon_t $$
$$ \Delta Y_t = \gamma y_t-1 + \varepsilon_t, \hspace{1em} \gamma := a_1 - 1 $$ 
Dickey and Fuller constructed the following regression equations:
% \begin{equation}
$$ \Delta y_t = \gamma y_{t-1} + \varepsilon_t $$
% \end{equation}

% \begin{equation}
$$ \Delta y_t = a_0 + \gamma y_{t-1} + \varepsilon_t $$
% \end{equation}

% \begin{equation}
$$ \Delta y_t = a_0 + \gamma y_{t-1} + a_2t + \varepsilon_t $$
% \end{equation}

(1) has no intercept and represents a simple random walk. (2) includes an intercept. (3) also adds a deterministic time trend. 

Note that the critical values of the t-statistics are *different* between these regressions. This will now be shown with our Monte Carlo simulation.

For equation (1):


``` {r monte carlo 1, tidy = FALSE}

set.seed(76345)

# length of ts
T <- 100

# Loops
S <- 10000

e <- rnorm(T, 0, 1)

# As y_{t+1} = y_t + \varepsilon_t, we can write this as an MA(\infty) model. y_t = \sum \varepsilon_i. This is now done with the cumsum function.

y <- cumsum(e)

y <- vector()

y[1] = e[1]

for (i in (2:T)) {

    y[i] <- y[(i-1)] + e[i]

}

y <- as.ts(y)

autoplot(y) + theme_bw()

# Taking the first difference

y_diff <- diff(y)

# Regression of I(1) model

reg <- dynlm(y_diff ~ 0 + L(y,1)) # no lag (x1 = 0)

reg <-summary(reg)

reg$coefficients[1,3] # t value

# Loop

tau <- vector()

for (i in 1:S) {
  
  e <- rnorm(T,0,1)
  y <- cumsum(e)
  y <- as.ts(y)
  
  y_diff <- diff(y)
  
  reg <- summary(dynlm(y_diff ~ 0 + L(y, 1)))
  
  tau[i] <- reg$coefficients[1,3]
  
}

tau.df <- data.frame(tau)

ggplot(data = tau.df, aes(x = tau)) + geom_density(color = "blue") + stat_function(fun = dnorm, n = 101, args = c(mean = 0, sd = 1)) + theme_few()


jarque.bera.test(tau) # We reject H0 at the 1% significance level.

tau.ci <- quantile(tau, c(0.01, 0.05, 0.1))

tau.ci


```

For equation (2) -- with an intercept:

```{r monte carlo 2}

# Loop

tau_mu <- vector()

for (i in 1:S) {
  
  e <- rnorm(T,0,1)
  y <- cumsum(e)
  y <- as.ts(y)
  
  y_diff <- diff(y)
  
  reg <- summary(dynlm(y_diff ~ 1 + L(y, 1)))
  
  tau_mu[i] <- reg$coefficients[2,3]
  
}

tau_mu.df <- data.frame(tau_mu)

ggplot(data = tau_mu.df, aes(x = tau_mu)) + geom_density(color = "blue") + stat_function(fun = dnorm, n = 101, args = c(mean = 0, sd = 1)) + theme_few()


jarque.bera.test(tau_mu) # We reject H0 at the 1% significance level.

tau_mu.ci <- quantile(tau_mu, c(0.01, 0.05, 0.1))

tau_mu.ci



```

And finally, for equation (3) -- with an intercept and a deterministic time trend:

\section{Part 3: Applying the Dickey-Fuller test for GDP}

```{r monte carlo 3}

# Loop

tau_t <- vector()
time <- c(1:T)

for (i in 1:S) {
  
  e <- rnorm(T,0,1)
  y <- cumsum(e)
  y <- as.ts(y)
  
  y_diff <- diff(y)
  
  reg <- summary(dynlm(y_diff ~ 1 + L(y, 1) + time[-1])) # removed 1 dimension for no. of obs. 
  
  tau_t[i] <- reg$coefficients[2,3]
  
}

tau_t.df <- data.frame(tau_t)

ggplot(data = tau_t.df, aes(x = tau_t)) + geom_density(color = "blue") + stat_function(fun = dnorm, n = 101, args = c(mean = 0, sd = 1)) + theme_few()


jarque.bera.test(tau_t) # We reject H0 at the 1% significance level.

tau_t.ci <- quantile(tau_t, c(0.01, 0.05, 0.1))

tau_t.ci



```











